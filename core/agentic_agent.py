#!/usr/bin/env python3
"""
Adaptive Agent - Universal Adaptive Extraction for ANY Clinical Task
Version: 1.0.0
Author: Frederick Gyasi (gyasi@musc.edu)
Institution: Medical University of South Carolina, Biomedical Informatics Center

üéØ TRULY UNIVERSAL & AGENTIC SYSTEM:
   BOTH execution modes (STRUCTURED & ADAPTIVE) are autonomous agents that adapt to ANY
   clinical extraction task defined by your prompts and JSON schema.
   Not hardcoded for specific conditions - works for ALL clinical domains!

EXECUTION MODE: ADAPTIVE (Evolving Tasks)
PURPOSE: Use for evolving, complex tasks requiring iterative refinement
BEHAVIOR: LLM decides ‚Üí Calls tools ‚Üí Learns ‚Üí Iterates ‚Üí Adapts ‚Üí Extracts

REVOLUTIONARY ARCHITECTURE:
Continuous adaptive loop where LLM:
- Autonomously decides what tools to call based on YOUR task
- Iterates and learns from results dynamically
- Adapts strategy to YOUR extraction goals
- Works for ANY clinical domain (not task-specific)

KEY FEATURES:
- Continuous loop with PAUSE/RESUME states
- Native tool calling (OpenAI/Anthropic function calling API)
- **ASYNC TOOL EXECUTION** - 60-75% faster with parallel execution
- Multiple calls to same tool with different queries as needed
- Dynamic discovery: "That result tells me I need X next"
- Context-aware chaining based on previous tool results
- Universal across ALL clinical domains (not task-specific)

PERFORMANCE:
- Async/await for parallel tool execution (concurrent RAG + functions)
- Maintains conversation order while parallelizing execution
- Typical speedup: 60-75% vs sequential execution
"""

import json
import time
import re
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

from core.json_parser import JSONParser
from core.prompt_templates import format_schema_as_instructions
from core.logging_config import get_logger

logger = get_logger(__name__)


class AgenticState(Enum):
    """States in the agentic loop"""
    IDLE = "idle"
    ANALYZING = "analyzing"
    AWAITING_TOOL_RESULTS = "awaiting_tool_results"  # PAUSED state
    CONTINUING = "continuing"  # RESUMED state
    EXTRACTING = "extracting"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ToolCall:
    """Represents a single tool call"""
    id: str
    type: str  # 'rag', 'function', 'extras'
    name: str
    parameters: Dict[str, Any]
    purpose: Optional[str] = None


@dataclass
class ToolResult:
    """Represents a tool execution result"""
    tool_call_id: str
    type: str
    success: bool
    result: Any
    message: Optional[str] = None


@dataclass
class ConversationMessage:
    """Message in the conversation history"""
    role: str  # 'system', 'user', 'assistant', 'tool'
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    tool_call_id: Optional[str] = None  # For tool result messages
    name: Optional[str] = None  # For tool result messages


@dataclass
class AgenticContext:
    """Context for agentic execution"""
    clinical_text: str
    label_context: Optional[str]
    state: AgenticState
    conversation_history: List[ConversationMessage] = field(default_factory=list)
    iteration: int = 0
    max_iterations: int = 10  # CHANGED: Reduced from 20 to 10
    total_tool_calls: int = 0
    max_tool_calls: int = 50
    tool_calls_this_iteration: List[ToolCall] = field(default_factory=list)
    tool_results: List[ToolResult] = field(default_factory=list)
    final_output: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

    # Preprocessing tracking
    original_text: Optional[str] = None
    redacted_text: Optional[str] = None
    normalized_text: Optional[str] = None

    # Stall detection - track repeated tool calls
    tool_call_history: List[str] = field(default_factory=list)
    stall_counter: int = 0
    consecutive_no_progress: int = 0

    # PERFORMANCE: Conversation history management
    conversation_window_size: int = 20  # Keep only last 20 messages for context
    total_messages_sent: int = 0  # Track total messages for metrics


class AgenticAgent:
    """
    Truly Agentic Extraction System with Continuous Loop

    The LLM autonomously:
    - Analyzes clinical text
    - Decides what tools to call (if any)
    - Learns from tool results
    - Iterates: calls more tools or completes extraction
    - Adapts strategy based on what it discovers
    """

    def __init__(self, llm_manager, rag_engine, extras_manager, function_registry,
                 regex_preprocessor, app_state):
        """Initialize agentic agent"""
        self.llm_manager = llm_manager
        self.rag_engine = rag_engine
        self.extras_manager = extras_manager
        self.function_registry = function_registry
        self.regex_preprocessor = regex_preprocessor
        self.app_state = app_state

        self.context: Optional[AgenticContext] = None
        self.json_parser = JSONParser()

        logger.info("üéØ AdaptiveAgent v1.0.0 initialized - ADAPTIVE Mode (evolving tasks with ASYNC)")

    def extract(self, clinical_text: str, label_value: Optional[Any] = None) -> Dict[str, Any]:
        """
        Main agentic extraction with continuous loop

        Flow:
        1. Initialize context with clinical text
        2. Start conversation with LLM
        3. Loop:
           - LLM analyzes/continues
           - If LLM requests tools ‚Üí PAUSE ‚Üí Execute ‚Üí RESUME
           - If LLM outputs JSON ‚Üí Extract and complete
        4. Return result
        """
        try:
            # Initialize context
            label_context = self._get_label_context_string(label_value)
            preprocessed_text = self._preprocess_clinical_text(clinical_text)

            self.context = AgenticContext(
                clinical_text=preprocessed_text,
                label_context=label_context,
                state=AgenticState.IDLE,
                original_text=clinical_text
            )

            logger.info("=" * 80)
            logger.info("ADAPTIVE MODE EXTRACTION STARTED (v1.0.0 - Evolving Tasks & Async)")
            logger.info("=" * 80)

            # Build initial prompt
            initial_prompt = self._build_agentic_initial_prompt()

            # Add system message if supported
            if self._supports_system_messages():
                system_message = self._build_system_message()
                self.context.conversation_history.append(
                    ConversationMessage(role='system', content=system_message)
                )

            # Add user message
            self.context.conversation_history.append(
                ConversationMessage(role='user', content=initial_prompt)
            )

            # Start agentic loop
            self.context.state = AgenticState.ANALYZING
            extraction_complete = False

            while not extraction_complete and self.context.iteration < self.context.max_iterations:
                self.context.iteration += 1
                logger.info(f"=" * 60)
                logger.info(f"ITERATION {self.context.iteration}/{self.context.max_iterations}")
                logger.info(f"State: {self.context.state.value}")
                logger.info(f"Conversation history size: {len(self.context.conversation_history)} messages")
                logger.info(f"Total tool calls so far: {self.context.total_tool_calls}")
                logger.info(f"Tool results collected: {len(self.context.tool_results)}")
                logger.info(f"=" * 60)

                # LLM generates response (may include tool calls or final JSON)
                logger.debug(f"ü§ñ Calling LLM for iteration {self.context.iteration}...")
                response = self._generate_with_tools()
                logger.debug(f"üì• LLM response received")

                if response is None:
                    logger.error("LLM returned no response")
                    break

                # Parse response
                has_tool_calls, has_json_output = self._parse_llm_response(response)

                if has_tool_calls:
                    # PAUSE - Execute tools
                    logger.info(f"LLM requested {len(self.context.tool_calls_this_iteration)} tools")
                    logger.debug(f"üìã Tool calls requested:")
                    for i, tc in enumerate(self.context.tool_calls_this_iteration):
                        logger.debug(f"  {i+1}. {tc.name}({json.dumps(tc.parameters)})")

                    self.context.state = AgenticState.AWAITING_TOOL_RESULTS

                    # Track tool calls for stall detection and duplicate detection
                    tool_signature = self._get_tool_calls_signature(self.context.tool_calls_this_iteration)
                    self.context.tool_call_history.append(tool_signature)
                    logger.debug(f"üîñ Tool signature: {tool_signature}")

                    # Detect duplicate function calls with same parameters
                    duplicates = self._detect_duplicate_function_calls(self.context.tool_calls_this_iteration)
                    if duplicates:
                        logger.warning(f"‚ö†Ô∏è DUPLICATE CALCULATIONS DETECTED: {len(duplicates)} functions called with same parameters as before")
                        for dup in duplicates:
                            logger.warning(f"   ‚Ä¢ {dup['function']}({dup['params']}) - already calculated in previous iteration")

                    # Check for repeated tool calls (stall detection)
                    if len(self.context.tool_call_history) >= 3:
                        last_three = self.context.tool_call_history[-3:]
                        if last_three[0] == last_three[1] == last_three[2]:
                            self.context.stall_counter += 1
                            logger.warning(f"‚ö†Ô∏è STALL DETECTED: Same tools called 3 times in a row (stall count: {self.context.stall_counter})")

                    tool_results = self._execute_tools(self.context.tool_calls_this_iteration)

                    # RESUME - Add tool results to conversation
                    logger.info(f"Tools executed, resuming with {len(tool_results)} results")
                    self.context.state = AgenticState.CONTINUING
                    self.context.consecutive_no_progress = 0  # Reset no-progress counter

                    # Add tool result messages
                    for result in tool_results:
                        self.context.conversation_history.append(
                            ConversationMessage(
                                role='tool',
                                tool_call_id=result.tool_call_id,
                                name=result.type,
                                content=self._format_tool_result_for_llm(result)
                            )
                        )

                    # Clear tool calls for next iteration
                    self.context.tool_calls_this_iteration = []

                    # If stalled too many times, force completion aggressively
                    if self.context.stall_counter >= 2:
                        logger.warning("üî¥ FORCING COMPLETION: Agent stalled - same tools called repeatedly")
                        logger.warning("üî¥ Next response MUST be valid JSON - no tool calls allowed")

                        # Try to extract JSON from current responses first
                        extracted = self._extract_json_from_conversation()
                        if extracted:
                            logger.info("‚úÖ Extracted JSON during force - completing immediately")
                            self.context.final_output = extracted
                            self.context.state = AgenticState.COMPLETED
                            extraction_complete = True
                        else:
                            # Send very strong forcing message
                            self.context.conversation_history.append(
                                ConversationMessage(
                                    role='user',
                                    content="""STOP CALLING TOOLS. You have all the information you need.

OUTPUT ONLY VALID JSON matching the schema. NO TOOL CALLS. NO EXPLANATIONS. ONLY JSON.

Format:
{
  "field1": "value1",
  "field2": "value2"
}

If you call tools again, the task will FAIL. Output JSON NOW."""
                                )
                            )
                            # Set flag to refuse tool calls on next iteration
                            self.context.stall_counter = 0  # Reset but next iteration will check for JSON only

                    # Loop continues - LLM can request more tools or finish

                elif has_json_output:
                    # Extraction complete
                    logger.info("‚úÖ LLM provided final JSON output - extraction complete")
                    self.context.state = AgenticState.COMPLETED
                    extraction_complete = True

                else:
                    # LLM is thinking/analyzing without calling tools or outputting JSON
                    self.context.consecutive_no_progress += 1
                    logger.info(f"LLM thinking/analyzing without progress (count: {self.context.consecutive_no_progress})")

                    remaining_iters = self.context.max_iterations - self.context.iteration

                    # If too many iterations with no progress OR near max iterations, force completion
                    if self.context.consecutive_no_progress >= 2 or remaining_iters <= 1:
                        logger.warning(f"üî¥ FORCING COMPLETION: No progress detected (consecutive={self.context.consecutive_no_progress}, remaining={remaining_iters})")

                        # Try to extract any existing JSON
                        extracted = self._extract_json_from_conversation()
                        if extracted:
                            logger.info("‚úÖ Extracted JSON during no-progress force - completing")
                            self.context.final_output = extracted
                            self.context.state = AgenticState.COMPLETED
                            extraction_complete = True
                        else:
                            self.context.conversation_history.append(
                                ConversationMessage(
                                    role='user',
                                    content="""You must complete the task NOW.

OUTPUT FINAL JSON using the schema provided.
NO MORE TOOL CALLS.
NO EXPLANATIONS.
ONLY JSON.

Example format:
{
  "field_name": "extracted_value"
}"""
                                )
                            )
                            self.context.consecutive_no_progress = 0
                    else:
                        # Add a prompt to encourage either tool use or completion
                        self.context.conversation_history.append(
                            ConversationMessage(
                                role='user',
                                content=f"Continue: Either (1) call tools to gather more information, OR (2) output the final JSON if you have sufficient information. {remaining_iters} iterations remaining."
                            )
                        )

            if self.context.iteration >= self.context.max_iterations:
                logger.warning(f"‚ö†Ô∏è Max iterations ({self.context.max_iterations}) reached")

                # Try to extract any JSON from the last few responses
                logger.info("üîç Attempting to extract JSON from conversation history...")
                extracted_json = self._extract_json_from_conversation()

                if extracted_json:
                    logger.info("‚úÖ Successfully extracted JSON from conversation")
                    self.context.final_output = extracted_json
                    self.context.state = AgenticState.COMPLETED
                else:
                    logger.warning("‚ùå No valid JSON found in conversation - marking as failed")
                    self.context.state = AgenticState.FAILED
                    self.context.error = "Max iterations reached without valid JSON output"

            # Final state logging for clear indication of success/failure
            logger.info("=" * 80)
            if self.context.state == AgenticState.COMPLETED:
                if self.context.final_output:
                    logger.info("üéâ EXTRACTION SUCCESS - Agent completed with valid JSON output")
                    logger.info(f"   Iterations: {self.context.iteration}/{self.context.max_iterations}")
                    logger.info(f"   Tool calls: {self.context.total_tool_calls}")
                    logger.info(f"   Final JSON fields: {list(self.context.final_output.keys()) if isinstance(self.context.final_output, dict) else 'N/A'}")
                else:
                    logger.warning("‚ö†Ô∏è EXTRACTION INCOMPLETE - Agent completed but no JSON output generated")
                    logger.warning(f"   Iterations: {self.context.iteration}/{self.context.max_iterations}")
            elif self.context.state == AgenticState.FAILED:
                logger.error("‚ùå EXTRACTION FAILED - Agent did not complete successfully")
                logger.error(f"   Reason: {self.context.error}")
                logger.error(f"   Iterations: {self.context.iteration}/{self.context.max_iterations}")
            else:
                logger.info(f"‚ÑπÔ∏è EXTRACTION ENDED - Agent state: {self.context.state.value}")
            logger.info("=" * 80)

            return self._build_extraction_result()

        except Exception as e:
            logger.error(f"Agentic extraction failed: {e}", exc_info=True)
            if self.context:
                self.context.state = AgenticState.FAILED
                self.context.error = str(e)
            return self._build_failure_result(str(e))

    def _generate_with_tools(self) -> Optional[Dict[str, Any]]:
        """
        Generate LLM response with tool calling support

        Returns dict with:
        - content: Text content
        - tool_calls: List of tool calls (if any)
        - finish_reason: 'stop', 'tool_calls', etc.
        """
        try:
            # Check if provider supports native tool calling
            if self._supports_native_tool_calling():
                return self._generate_with_native_tool_calling()
            else:
                # Fallback to text-based tool calling
                return self._generate_with_text_based_tools()

        except Exception as e:
            logger.error(f"Generation with tools failed: {e}", exc_info=True)
            return None

    def _supports_native_tool_calling(self) -> bool:
        """Check if provider supports native function calling"""
        return self.llm_manager.provider in ['openai', 'anthropic']

    def _supports_system_messages(self) -> bool:
        """Check if provider supports system messages"""
        return self.llm_manager.provider in ['openai', 'anthropic', 'google']

    def _generate_with_native_tool_calling(self) -> Dict[str, Any]:
        """Generate using native tool calling API"""
        # Use enhanced llm_manager method
        return self.llm_manager.generate_with_tool_calling(
            messages=self._convert_history_to_api_format(),
            tools=self._get_tool_schema(),
            max_tokens=self.app_state.model_config.max_tokens
        )

    def _generate_with_text_based_tools(self) -> Dict[str, Any]:
        """
        Fallback: Text-based tool calling for providers that don't support native tools

        ENHANCED: Includes tool descriptions in prompt so LLM knows what tools exist
        """
        # Build comprehensive prompt with tool descriptions
        messages = self._convert_history_to_text_with_tools()

        # Generate
        response_text = self.llm_manager.generate(
            messages,
            max_tokens=self.app_state.model_config.max_tokens
        )

        # Parse for tool calls or JSON output
        return self._parse_text_response_for_tools(response_text)

    def _get_tool_schema(self) -> List[Dict[str, Any]]:
        """
        Get tool schema in OpenAI/Anthropic function calling format
        """
        tools = []

        # RAG tool
        if self.rag_engine:
            tools.append({
                'type': 'function',
                'function': {
                    'name': 'query_rag',
                    'description': 'Query retrieval system for guidelines, standards, criteria, and reference information from authoritative sources. Can be called MULTIPLE times with different queries to refine information.',
                    'parameters': {
                        'type': 'object',
                        'properties': {
                            'query': {
                                'type': 'string',
                                'description': 'Focused query with 4-8 specific keywords targeting guidelines, diagnostic criteria, standards, or reference information'
                            },
                            'purpose': {
                                'type': 'string',
                                'description': 'Why you need this information (helps with debugging)'
                            }
                        },
                        'required': ['query']
                    }
                }
            })

        # Function tool
        if self.function_registry:
            # Get all available functions
            functions = self.function_registry.get_all_functions_info()
            for func in functions:
                tools.append({
                    'type': 'function',
                    'function': {
                        'name': f"call_{func['name']}",
                        'description': f"{func.get('description', 'Calculation or transformation function')}. Can be called multiple times with different inputs.",
                        'parameters': {
                            'type': 'object',
                            'properties': func.get('parameters', {}),
                            'required': list(func.get('parameters', {}).keys())
                        }
                    }
                })

        # Extras tool
        if self.extras_manager:
            tools.append({
                'type': 'function',
                'function': {
                    'name': 'query_extras',
                    'description': 'Query supplementary hints/tips/patterns that help understand the task. Use specific keywords to match relevant hints and guidance.',
                    'parameters': {
                        'type': 'object',
                        'properties': {
                            'keywords': {
                                'type': 'array',
                                'items': {'type': 'string'},
                                'description': '3-5 specific keywords related to your task (e.g., domain-specific terms, concepts, assessment types, field names from schema)'
                            }
                        },
                        'required': ['keywords']
                    }
                }
            })

        return tools

    def _extract_json_from_conversation(self) -> Optional[Dict[str, Any]]:
        """
        Try to extract valid JSON from conversation history (last 3 assistant messages)

        This is a fallback when max iterations is reached
        """
        # Check last 3 assistant messages for any JSON
        for msg in reversed(self.context.conversation_history[-6:]):  # Check last 6 messages
            if msg.role == 'assistant' and msg.content:
                parsed = self._try_parse_json(msg.content)
                if parsed:
                    logger.info(f"üìù Found valid JSON in assistant message: {str(parsed)[:100]}...")
                    return parsed

        # No valid JSON found
        return None

    def _detect_duplicate_function_calls(self, current_calls: List[ToolCall]) -> List[Dict[str, str]]:
        """
        Detect if any function calls are duplicates of previous calls

        Returns list of duplicates with function name and parameters
        """
        duplicates = []

        # Get all previous function calls from tool_results
        previous_calls = {}
        for result in self.context.tool_results:
            if result.type == 'function' and result.success:
                # Find the original tool call
                for msg in self.context.conversation_history:
                    if msg.role == 'assistant' and msg.tool_calls:
                        for tc in msg.tool_calls:
                            if tc.id == result.tool_call_id:
                                # Create signature: function_name + sorted parameters
                                func_name = tc.name.replace('call_', '')
                                param_sig = json.dumps(tc.parameters, sort_keys=True)
                                call_sig = f"{func_name}:{param_sig}"
                                previous_calls[call_sig] = {
                                    'function': func_name,
                                    'params': tc.parameters,
                                    'result': result.result
                                }

        # Check current calls against previous
        for tc in current_calls:
            if tc.name.startswith('call_'):
                func_name = tc.name.replace('call_', '')
                param_sig = json.dumps(tc.parameters, sort_keys=True)
                call_sig = f"{func_name}:{param_sig}"

                if call_sig in previous_calls:
                    duplicates.append({
                        'function': func_name,
                        'params': str(tc.parameters),
                        'previous_result': previous_calls[call_sig]['result']
                    })

        return duplicates

    def _get_tool_calls_signature(self, tool_calls: List[ToolCall]) -> str:
        """
        Create a signature for tool calls to detect repeated patterns

        Returns a string representing the tools and their key parameters
        """
        signatures = []
        for tc in tool_calls:
            # Create signature: tool_name + sorted parameter keys
            param_keys = sorted(tc.parameters.keys()) if tc.parameters else []
            sig = f"{tc.name}({','.join(param_keys)})"
            signatures.append(sig)

        return ";".join(sorted(signatures))

    def _execute_tools(self, tool_calls: List[ToolCall]) -> List[ToolResult]:
        """
        Execute all tool calls in PARALLEL using async

        Phase 2: Tools run concurrently for better performance
        - Multiple RAG queries can run simultaneously
        - Multiple function calls can run simultaneously
        - Results returned in original order for conversation coherence
        """
        logger.info(f"Executing {len(tool_calls)} tools in PARALLEL (async)")

        # Run async execution in event loop
        try:
            # Try to get existing event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is already running (e.g., in Jupyter), use nest_asyncio or run_until_complete
                results = loop.run_until_complete(self._execute_tools_async(tool_calls))
            else:
                results = loop.run_until_complete(self._execute_tools_async(tool_calls))
        except RuntimeError:
            # No event loop, create new one
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(self._execute_tools_async(tool_calls))

        # Update tracking
        for result in results:
            self.context.tool_results.append(result)
            self.context.total_tool_calls += 1

        return results

    def _deduplicate_tool_calls(self, tool_calls: List[ToolCall]) -> List[ToolCall]:
        """
        Deduplicate tool calls to prevent executing identical calls multiple times.

        Two tool calls are considered identical if they have:
        - Same name
        - Same parameters (compared as JSON strings for deterministic comparison)

        This prevents the agent from getting stuck calling the same function repeatedly
        with identical parameters, which is a common LLM behavior issue.

        Returns:
            List of unique tool calls (keeps first occurrence of each duplicate)
        """
        seen = {}
        unique_calls = []
        duplicates_removed = 0

        # DIAGNOSTIC: Log all tool calls being checked
        logger.debug(f"üîç Deduplication check for {len(tool_calls)} tool calls:")
        for i, tc in enumerate(tool_calls):
            params_json = json.dumps(tc.parameters, sort_keys=True)
            logger.debug(f"  {i+1}. {tc.name} with params: {params_json}")

        for tool_call in tool_calls:
            # Create a unique key based on name and parameters
            # Sort parameters for consistent comparison
            params_json = json.dumps(tool_call.parameters, sort_keys=True)
            key = f"{tool_call.name}||{params_json}"

            logger.debug(f"üîë Dedup key: {key}")

            if key not in seen:
                seen[key] = tool_call
                unique_calls.append(tool_call)
                logger.debug(f"  ‚úÖ UNIQUE - keeping this call")
            else:
                duplicates_removed += 1
                logger.warning(
                    f"‚ö†Ô∏è DUPLICATE TOOL CALL detected and skipped: "
                    f"{tool_call.name} with parameters {params_json}"
                )
                logger.debug(f"  ‚ùå DUPLICATE - already seen this exact call")

        if duplicates_removed > 0:
            logger.warning(
                f"‚ö†Ô∏è Removed {duplicates_removed} duplicate tool calls "
                f"(kept {len(unique_calls)} unique calls from {len(tool_calls)} total)"
            )

        return unique_calls

    async def _execute_tools_async(self, tool_calls: List[ToolCall]) -> List[ToolResult]:
        """
        Async execution of tool calls - runs in parallel

        This is the Phase 2 enhancement that significantly improves performance

        IMPORTANT: Deduplicates tool calls before execution to prevent
        the agent from getting stuck calling the same function repeatedly.
        """
        # CRITICAL FIX: Deduplicate tool calls before execution
        original_count = len(tool_calls)
        tool_calls = self._deduplicate_tool_calls(tool_calls)

        if len(tool_calls) == 0:
            logger.warning("‚ö†Ô∏è No tool calls to execute after deduplication")
            return []

        # Create tasks for all tools
        tasks = []
        for tool_call in tool_calls:
            # CRITICAL: Check by NAME first! All tools have type='function' from native APIs
            # So we must check specific names (query_rag, query_extras) before checking call_ prefix
            if tool_call.name == 'query_rag':
                task = self._execute_rag_tool_async(tool_call)
            elif tool_call.name == 'query_extras':
                task = self._execute_extras_tool_async(tool_call)
            elif tool_call.name.startswith('call_') or tool_call.type == 'function':
                task = self._execute_function_tool_async(tool_call)
            else:
                # Unknown tool type - create a coroutine that returns error
                async def unknown_tool():
                    return ToolResult(
                        tool_call_id=tool_call.id,
                        type=tool_call.type,
                        success=False,
                        result=None,
                        message=f"Unknown tool type: {tool_call.type}"
                    )
                task = unknown_tool()

            tasks.append(task)

        # Execute all tasks in parallel
        start_time = time.time()
        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start_time

        logger.info(f"‚úÖ Executed {len(tool_calls)} tools in {elapsed:.2f}s (parallel)")

        return results

    def _execute_rag_tool(self, tool_call: ToolCall) -> ToolResult:
        """Execute RAG query"""
        try:
            # Check if RAG engine is available
            if not self.rag_engine:
                error_msg = (
                    "‚ùå RAG Engine Not Initialized\n\n"
                    "The agent requested RAG (document retrieval), but RAG is not configured.\n\n"
                    "To enable RAG:\n"
                    "1. Go to the 'RAG' tab\n"
                    "2. Upload your documents (PDFs, text files, etc.)\n"
                    "3. Click 'Build Index' to create the vector database\n"
                    "4. Ensure 'Enable RAG' is checked in RAG configuration\n\n"
                    "Without RAG, the agent will rely only on Functions and Extras for knowledge."
                )
                logger.warning(f"‚ö†Ô∏è RAG engine not available - agent requested RAG but it's not initialized")
                logger.info("üìñ To fix: Upload documents in RAG tab ‚Üí Build Index ‚Üí Enable RAG")
                return ToolResult(
                    tool_call_id=tool_call.id,
                    type='rag',
                    success=False,
                    result=[],
                    message=error_msg
                )

            query = tool_call.parameters.get('query', '')

            if not query or len(query) < 5:
                logger.warning(f"‚ùå RAG query rejected: Query too short or empty (length={len(query)})")
                return ToolResult(
                    tool_call_id=tool_call.id,
                    type='rag',
                    success=False,
                    result=[],
                    message='Query too short or empty'
                )

            logger.info(f"üîç RAG query: '{query}'")

            top_k = self.app_state.rag_config.rag_top_k
            results = self.rag_engine.query(query_text=query, k=top_k)

            if len(results) > 0:
                logger.info(f"‚úÖ RAG retrieved {len(results)} documents (top_k={top_k})")
            else:
                logger.warning(f"‚ö†Ô∏è RAG found no results for query: '{query}'")

            return ToolResult(
                tool_call_id=tool_call.id,
                type='rag',
                success=len(results) > 0,
                result=results,
                message=f"Retrieved {len(results)} documents"
            )

        except Exception as e:
            logger.error(f"‚ùå RAG execution failed: {e}", exc_info=True)
            return ToolResult(
                tool_call_id=tool_call.id,
                type='rag',
                success=False,
                result=[],
                message=str(e)
            )

    def _execute_function_tool(self, tool_call: ToolCall) -> ToolResult:
        """Execute function call"""
        try:
            # Check if function registry is available
            if not self.function_registry:
                logger.warning(f"‚ö†Ô∏è Function registry not available")
                return ToolResult(
                    tool_call_id=tool_call.id,
                    type='function',
                    success=False,
                    result=None,
                    message='Function registry not available'
                )

            # Extract function name (remove 'call_' prefix if present)
            func_name = tool_call.name
            if func_name.startswith('call_'):
                func_name = func_name[5:]

            parameters = tool_call.parameters

            logger.info(f"‚öôÔ∏è Calling function: {func_name}({', '.join(f'{k}={v}' for k, v in parameters.items())})")

            success, result, message = self.function_registry.execute_function(
                func_name, **parameters
            )

            if success:
                logger.info(f"‚úÖ Function {func_name} executed successfully: {result}")
            else:
                logger.warning(f"‚ùå Function {func_name} failed: {message}")

            return ToolResult(
                tool_call_id=tool_call.id,
                type='function',
                success=success,
                result=result,
                message=message
            )

        except Exception as e:
            logger.error(f"‚ùå Function execution failed with exception: {e}", exc_info=True)
            return ToolResult(
                tool_call_id=tool_call.id,
                type='function',
                success=False,
                result=None,
                message=str(e)
            )

    def _execute_extras_tool(self, tool_call: ToolCall) -> ToolResult:
        """Execute extras query"""
        try:
            # Check if extras manager is available
            if not self.extras_manager:
                logger.warning(f"‚ö†Ô∏è Extras manager not available")
                return ToolResult(
                    tool_call_id=tool_call.id,
                    type='extras',
                    success=False,
                    result=[],
                    message='Extras manager not available'
                )

            keywords = tool_call.parameters.get('keywords', [])

            logger.info(f"üí° Querying extras with keywords: {keywords}")

            matched_extras = self.extras_manager.match_extras_by_keywords(keywords)

            if len(matched_extras) > 0:
                logger.info(f"‚úÖ Matched {len(matched_extras)} extras hints/patterns")
                # Log first few matched extras for debugging
                for i, extra in enumerate(matched_extras[:3]):
                    extra_id = extra.get('id', 'N/A')
                    extra_type = extra.get('type', 'unknown')
                    logger.debug(f"   ‚Ä¢ Extra [{i+1}]: {extra_id} ({extra_type})")
            else:
                logger.warning(f"‚ö†Ô∏è No extras matched for keywords: {keywords}")

            return ToolResult(
                tool_call_id=tool_call.id,
                type='extras',
                success=len(matched_extras) > 0,
                result=matched_extras,
                message=f"Matched {len(matched_extras)} extras"
            )

        except Exception as e:
            logger.error(f"‚ùå Extras execution failed: {e}", exc_info=True)
            return ToolResult(
                tool_call_id=tool_call.id,
                type='extras',
                success=False,
                result=[],
                message=str(e)
            )

    # ========================================================================
    # ASYNC VERSIONS OF TOOL EXECUTION (Phase 2)
    # ========================================================================

    async def _execute_rag_tool_async(self, tool_call: ToolCall) -> ToolResult:
        """Execute RAG query asynchronously"""
        logger.info(f"[ASYNC] Executing RAG: {tool_call.parameters.get('query', '')[:50]}...")

        # Run in thread pool since RAG engine is synchronous
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,  # Use default executor
            self._execute_rag_tool,
            tool_call
        )
        return result

    async def _execute_function_tool_async(self, tool_call: ToolCall) -> ToolResult:
        """Execute function call asynchronously"""
        func_name = tool_call.name
        if func_name.startswith('call_'):
            func_name = func_name[5:]

        logger.info(f"[ASYNC] Executing function: {func_name}")

        # Run in thread pool since function registry is synchronous
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._execute_function_tool,
            tool_call
        )
        return result

    async def _execute_extras_tool_async(self, tool_call: ToolCall) -> ToolResult:
        """Execute extras query asynchronously"""
        keywords = tool_call.parameters.get('keywords', [])
        logger.info(f"[ASYNC] Executing extras: {keywords}")

        # Run in thread pool since extras manager is synchronous
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._execute_extras_tool,
            tool_call
        )
        return result

    # ========================================================================
    # END ASYNC TOOL EXECUTION
    # ========================================================================

    def _build_agentic_initial_prompt(self) -> str:
        """
        Build initial agentic prompt using USER'S task-specific prompt as PRIMARY

        ALWAYS tries main_prompt first, falls back to minimal_prompt only if main is missing.
        """
        from core.prompt_templates import get_agentic_extraction_prompt

        schema_instructions = format_schema_as_instructions(
            self.app_state.prompt_config.json_schema
        )

        # FIXED: Always try MAIN prompt first, fall back to MINIMAL only if MAIN is missing
        user_task_prompt = None

        if self.app_state.prompt_config.main_prompt:
            user_task_prompt = self.app_state.prompt_config.main_prompt
            logger.info("‚úì Using MAIN prompt as primary task definition")
        elif self.app_state.prompt_config.minimal_prompt:
            user_task_prompt = self.app_state.prompt_config.minimal_prompt
            logger.info("‚ö† Using MINIMAL prompt as fallback (main_prompt not set)")
        elif self.app_state.prompt_config.base_prompt:
            user_task_prompt = self.app_state.prompt_config.base_prompt
            logger.info("‚ö† Using BASE prompt as fallback (main and minimal not set)")
        else:
            user_task_prompt = ""
            logger.warning("‚ùå No user task-specific prompt found - using generic agentic prompt")

        prompt = get_agentic_extraction_prompt(
            clinical_text=self.context.clinical_text,
            label_context=self.context.label_context or "No label provided",
            json_schema=json.dumps(self.app_state.prompt_config.json_schema, indent=2),
            schema_instructions=schema_instructions,
            user_task_prompt=user_task_prompt
        )

        return prompt

    def _build_system_message(self) -> str:
        """Build system message for providers that support it"""
        return """You are an expert assistant performing information extraction from text.

You have access to tools:
- query_rag: Retrieve guidelines, standards, and reference information from authoritative sources
- call_[function]: Perform calculations and transformations
- query_extras: Get supplementary hints, patterns, and reference information

**UNIVERSAL ITERATIVE WORKFLOW:**

üî¥ CRITICAL: You MUST use an iterative, self-reflective approach! üî¥

**PHASE 1 - INITIAL ANALYSIS:**
1. Read the task prompt ‚Üí Understand what needs to be extracted
2. Read the input text ‚Üí Identify key metrics, measurements, entities, and information
3. Build queries and execute INITIAL tool calls:
   - Call functions for calculations or data transformations
   - Call query_extras for task-specific hints and patterns

**PHASE 2 - BUILD CONTEXT:**
4. Review function results and extras hints
5. Build RAG keywords based on what you learned (domain, criteria needed, guidelines)
6. Call query_rag to fetch relevant guidelines, standards, or reference information

**PHASE 3 - ASSESS INFORMATION GAPS:**
7. Determine: Do I have ALL information needed to complete the task?
   - ‚úÖ YES: Proceed to Phase 4
   - ‚ùå NO: Go to Phase 3b

**PHASE 3b - FILL GAPS:**
8. Identify what other information is needed
9. Determine which tools to call again (can call same tool with different queries)
10. Fetch additional information ‚Üí Return to Phase 3

**PHASE 4 - COMPLETION:**
11. When you have all necessary information ‚Üí Output final JSON extraction using the provided schema

**üî¥ CRITICAL REQUIREMENTS - MUST FOLLOW:**

**1. USE ALL TOOL RESULTS IN YOUR FINAL OUTPUT:**
   - EVERY function result MUST appear in your JSON output
   - EVERY RAG document MUST inform your reasoning and content
   - EVERY extras hint MUST be applied to your extraction
   - ‚ùå DO NOT call tools "for fun" - use every result!

**2. NO DUPLICATE CALCULATIONS:**
   - TRACK what you've already calculated
   - DO NOT call the same function with the same parameters twice
   - If you need a value you already calculated, REMEMBER it from the tool result
   - Example: If you called percentile_to_zscore(14) and got 0.72, DO NOT call it again

**3. INCORPORATE RETRIEVED INFORMATION:**
   - When RAG returns criteria/guidelines ‚Üí CITE them in your output
   - When extras returns hints ‚Üí APPLY them to your extraction
   - When functions return values ‚Üí INCLUDE the exact values in your JSON

**Key Principles:**
- ‚ùå DO NOT output final JSON immediately without gathering information
- ‚ùå DO NOT recalculate values you already have
- ‚ùå DO NOT ignore tool results - USE EVERY ONE
- ‚úÖ Call tools iteratively across multiple rounds
- ‚úÖ Use tool results to inform next tool calls
- ‚úÖ Remember and reuse calculated values
- ‚úÖ Cite RAG sources and apply extras guidance
- ‚úÖ Output ONLY valid JSON - no explanations or markdown

**CRITICAL: Tool Call Format**
- Tool calls MUST start with "TOOL_CALL:" prefix
- Format: TOOL_CALL: {{"tool": "name", "parameters": {{...}}}}
- Do NOT output bare JSON without the TOOL_CALL: prefix"""

    def _format_tool_result_for_llm(self, result: ToolResult) -> str:
        """Format tool result for LLM consumption"""
        if not result.success:
            return f"Tool execution failed: {result.message}"

        if result.type == 'rag':
            # Format RAG results
            chunks = []
            for chunk in result.result[:5]:  # Top 5
                content = chunk.get('content', '') or chunk.get('text', '')
                source = chunk.get('metadata', {}).get('source', 'unknown') if isinstance(chunk.get('metadata'), dict) else 'unknown'
                chunks.append(f"[Source: {source}]\n{content}\n")
            return "\n\n".join(chunks)

        elif result.type == 'function':
            return f"Result: {json.dumps(result.result, indent=2)}\n{result.message or ''}"

        elif result.type == 'extras':
            # Format extras
            extras_text = []
            for extra in result.result:
                content = extra.get('content', '')
                extras_text.append(content)
            return "\n\n".join(extras_text)

        else:
            return str(result.result)

    def _parse_llm_response(self, response: Dict[str, Any]) -> Tuple[bool, bool]:
        """
        Parse LLM response to determine next action

        Returns:
            (has_tool_calls, has_json_output)
        """
        # Check for tool calls
        tool_calls = response.get('tool_calls', [])
        if tool_calls:
            # Parse and store tool calls
            for tc in tool_calls:
                self.context.tool_calls_this_iteration.append(
                    ToolCall(
                        id=tc.get('id', f"call_{time.time()}"),
                        type=tc.get('type', 'function'),
                        name=tc.get('function', {}).get('name', ''),
                        parameters=json.loads(tc.get('function', {}).get('arguments', '{}')),
                        purpose=tc.get('purpose')
                    )
                )

            # Add assistant message to history
            self.context.conversation_history.append(
                ConversationMessage(
                    role='assistant',
                    content=response.get('content'),
                    tool_calls=self.context.tool_calls_this_iteration.copy()
                )
            )

            return True, False

        # Check for JSON output
        content = response.get('content', '')
        if content:
            # Try to parse JSON
            parsed_json = self._try_parse_json(content)
            if parsed_json:
                self.context.final_output = parsed_json
                # Add to history
                self.context.conversation_history.append(
                    ConversationMessage(role='assistant', content=content)
                )
                return False, True
            else:
                # Just thinking/analyzing OR invalid JSON was rejected
                self.context.conversation_history.append(
                    ConversationMessage(role='assistant', content=content)
                )

                # Check if this looks like a malformed tool call or invalid JSON
                if '{' in content and '}' in content:
                    # Looks like JSON but was rejected - provide feedback
                    logger.info("LLM output contains JSON but was rejected (schema validation failed)")
                    # The loop will continue and prompt the LLM again

                return False, False

        return False, False

    def _try_parse_json(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Try to parse JSON from text with schema validation

        CRITICAL: Validates that parsed JSON matches expected schema fields
        to prevent accidentally accepting tool call JSON as final output
        """
        parsed, method = self.json_parser.parse_json_response(
            text,
            self.app_state.prompt_config.json_schema
        )

        if not parsed:
            return None

        # CRITICAL VALIDATION: Check if this is actually a tool call JSON (wrong format)
        # Tool calls have format: {"tool": "...", "parameters": {...}}
        if isinstance(parsed, dict) and 'tool' in parsed and 'parameters' in parsed:
            logger.warning("‚ö†Ô∏è Detected tool call JSON without TOOL_CALL: prefix - rejecting as final output")
            logger.warning(f"LLM must use format: TOOL_CALL: {{\"tool\": \"...\", \"parameters\": {{...}}}}")
            return None

        # CRITICAL VALIDATION: Verify parsed JSON has expected schema fields
        schema = self.app_state.prompt_config.json_schema
        if schema and isinstance(parsed, dict):
            # Get required fields from schema
            required_fields = [
                field for field, props in schema.items()
                if isinstance(props, dict) and props.get('required', False)
            ]

            # Check if at least one required field is present
            if required_fields:
                has_required = any(field in parsed for field in required_fields)
                if not has_required:
                    logger.warning(f"‚ö†Ô∏è Parsed JSON missing all required fields: {required_fields}")
                    logger.warning(f"Parsed JSON keys: {list(parsed.keys())}")
                    return None

        return parsed

    def _apply_conversation_window(self, messages: List[ConversationMessage]) -> List[ConversationMessage]:
        """
        Apply SMART sliding window to conversation history for performance

        CRITICAL FIX: Preserves ALL tool results so LLM knows what was completed!

        CRITICAL FOR ALL PROVIDERS: Limits context size to prevent:
        1. Exponential slowdown as iterations increase
        2. Token context overflow
        3. Redundant re-processing of old messages

        SMART Strategy (FIXED - preserves completed work):
        1. Always keep: System message (tool definitions)
        2. Always keep: Initial user message (task + input text)
        3. Always keep: ALL tool result messages (completed work - CRITICAL!)
        4. Always keep: ALL assistant messages with tool_calls (what was requested)
        5. Keep: Recent other messages (user prompts, assistant thinking)
        6. Drop: Old assistant thinking messages with no tool calls

        Why this works:
        - LLM always sees what tools were called (assistant with tool_calls)
        - LLM always sees results of those tools (tool result messages)
        - LLM doesn't need old thinking messages that led to those calls
        """
        if len(messages) <= self.context.conversation_window_size:
            return messages

        # Categorize messages by importance
        system_msg = None
        initial_user_msg = None
        tool_result_messages = []  # CRITICAL: ALL tool results must be kept!
        assistant_with_tool_calls = []  # Keep: Shows what tools were called
        other_messages = []  # Can drop some of these

        for i, msg in enumerate(messages):
            if msg.role == 'system' and system_msg is None:
                system_msg = msg
            elif msg.role == 'user' and initial_user_msg is None:
                initial_user_msg = msg
            elif msg.role == 'tool':
                # CRITICAL: Always keep ALL tool results!
                # These contain completed calculations/retrievals
                tool_result_messages.append((i, msg))
            elif msg.role == 'assistant' and msg.tool_calls:
                # Keep assistant messages that made tool calls
                assistant_with_tool_calls.append((i, msg))
            else:
                # Other messages: user prompts, assistant thinking
                # These can be dropped if space is needed
                other_messages.append((i, msg))

        # Calculate available slots for "other" messages
        reserved_count = 0
        if system_msg:
            reserved_count += 1
        if initial_user_msg:
            reserved_count += 1
        reserved_count += len(tool_result_messages)  # All tool results (CRITICAL!)
        reserved_count += len(assistant_with_tool_calls)  # All tool call requests

        # How many slots left for other messages?
        # FIXED: Properly enforce window size while keeping some recent context
        available_slots = self.context.conversation_window_size - reserved_count
        if available_slots >= 5:
            # We have room for at least 5 thinking messages - great!
            other_message_limit = available_slots
        elif available_slots > 0:
            # Limited room (1-4 messages) - keep what we can
            other_message_limit = available_slots
        else:
            # Reserved messages already exceed window - keep 3 most recent for minimal context
            # This is a compromise: we MUST keep tool results, but need SOME thinking context
            other_message_limit = 3
            logger.debug(
                f"‚ö†Ô∏è Window size exceeded by reserved messages: "
                f"{reserved_count} reserved vs {self.context.conversation_window_size} limit. "
                f"Keeping {other_message_limit} recent thinking messages anyway."
            )

        # Keep most recent "other" messages
        if len(other_messages) > other_message_limit:
            other_messages = other_messages[-other_message_limit:]

        # Reconstruct conversation in original chronological order
        all_kept = []
        if system_msg:
            all_kept.append((0, system_msg))
        if initial_user_msg:
            all_kept.append((1, initial_user_msg))
        all_kept.extend(tool_result_messages)
        all_kept.extend(assistant_with_tool_calls)
        all_kept.extend(other_messages)

        # Sort by original index to maintain conversation flow
        all_kept.sort(key=lambda x: x[0])
        windowed = [msg for idx, msg in all_kept]

        # Log windowing for performance tracking
        if len(messages) > len(windowed):
            dropped_count = len(messages) - len(windowed)
            logger.info(f"üîÑ SMART windowing: {len(messages)} ‚Üí {len(windowed)} messages")
            logger.info(f"   ‚úÖ Preserved: {len(tool_result_messages)} tool results (LLM knows what was completed)")
            logger.info(f"   ‚úÖ Preserved: {len(assistant_with_tool_calls)} tool call requests")
            logger.info(f"   üóëÔ∏è Dropped: {dropped_count} old thinking/prompt messages")

        return windowed

    def _convert_history_to_api_format(self) -> List[Dict[str, Any]]:
        """Convert conversation history to API format with windowing"""
        # PERFORMANCE: Apply conversation window for ALL providers (OpenAI, Anthropic, Google, Azure, Local)
        windowed_history = self._apply_conversation_window(self.context.conversation_history)

        # Track metrics
        self.context.total_messages_sent += len(windowed_history)

        messages = []

        for msg in windowed_history:
            if msg.role == 'system':
                messages.append({'role': 'system', 'content': msg.content})
            elif msg.role == 'user':
                messages.append({'role': 'user', 'content': msg.content})
            elif msg.role == 'assistant':
                api_msg = {'role': 'assistant'}
                if msg.content:
                    api_msg['content'] = msg.content
                if msg.tool_calls:
                    api_msg['tool_calls'] = [
                        {
                            'id': tc.id,
                            'type': 'function',
                            'function': {
                                'name': tc.name,
                                'arguments': json.dumps(tc.parameters)
                            }
                        }
                        for tc in msg.tool_calls
                    ]
                messages.append(api_msg)
            elif msg.role == 'tool':
                messages.append({
                    'role': 'tool',
                    'tool_call_id': msg.tool_call_id,
                    'content': msg.content
                })

        return messages

    def _convert_history_to_text(self) -> str:
        """Convert conversation history to text for non-tool-calling providers with windowing"""
        # PERFORMANCE: Apply conversation window (same as API format)
        windowed_history = self._apply_conversation_window(self.context.conversation_history)

        lines = []
        for msg in windowed_history:
            if msg.role == 'system':
                lines.append(f"SYSTEM: {msg.content}")
            elif msg.role == 'user':
                lines.append(f"USER: {msg.content}")
            elif msg.role == 'assistant':
                lines.append(f"ASSISTANT: {msg.content}")
            elif msg.role == 'tool':
                lines.append(f"TOOL RESULT: {msg.content}")
        return "\n\n".join(lines)

    def _convert_history_to_text_with_tools(self) -> str:
        """
        Convert conversation history to text WITH comprehensive tool descriptions

        CRITICAL for local models to understand what tools are available and how to use them
        """
        lines = []

        # Add system message with tool descriptions
        system_msg = self._build_system_message()
        lines.append(f"SYSTEM: {system_msg}")

        # Add comprehensive tool guide
        lines.append("\n" + "=" * 80)
        lines.append("üìö TOOL GUIDE - Use Tools Before Final Extraction")
        lines.append("=" * 80)

        lines.append("\nüîç **THREE TOOL CATEGORIES:**\n")

        lines.append("1Ô∏è‚É£ **RAG (Retrieval-Augmented Generation)** - query_rag()")
        lines.append("   ‚Ä¢ Retrieves guidelines, diagnostic criteria, standards, and reference information")
        lines.append("   ‚Ä¢ Build queries from: domain + specificity + what you need")
        lines.append("   ‚Ä¢ Example: 'diagnostic criteria for condition X' or 'reference standards for metric Y'\n")

        lines.append("2Ô∏è‚É£ **FUNCTIONS (Calculations & Transformations)** - call_[function_name]()")
        lines.append("   ‚Ä¢ Performs calculations and data transformations (see full list below)")
        lines.append("   ‚Ä¢ Scan text for measurements and data, match to function parameters")
        lines.append("   ‚Ä¢ CRITICAL: Call functions BEFORE making interpretations\n")

        lines.append("3Ô∏è‚É£ **EXTRAS (Supplementary Hints)** - query_extras()")
        lines.append("   ‚Ä¢ Task-specific hints, patterns, reference ranges, and interpretation guides")
        lines.append("   ‚Ä¢ Build keywords from: schema field names + domain + relevant terms")
        lines.append("   ‚Ä¢ Use 3-5 specific terms relevant to your task, avoid generic words\n")

        # Add detailed tool descriptions
        lines.append("=" * 80)
        lines.append("üîß AVAILABLE TOOLS WITH PARAMETERS:")
        lines.append("=" * 80 + "\n")

        tools = self._get_tool_schema()
        for tool in tools:
            func = tool['function']
            lines.append(f"üìå **{func['name']}**")
            lines.append(f"   {func['description']}")

            # Format parameters more clearly
            params = func['parameters'].get('properties', {})
            if params:
                lines.append("   PARAMETERS:")
                for param_name, param_info in params.items():
                    param_type = param_info.get('type', 'any')
                    param_desc = param_info.get('description', 'No description')
                    lines.append(f"     ‚Ä¢ {param_name} ({param_type}): {param_desc}")
            lines.append("")

        # Add clear instructions with examples
        lines.append("=" * 80)
        lines.append("üìù HOW TO CALL TOOLS:")
        lines.append("=" * 80)
        lines.append("\nüî¥ CRITICAL: Tool calls MUST start with 'TOOL_CALL:' prefix! üî¥\n")
        lines.append("REQUIRED FORMAT:")
        lines.append('TOOL_CALL: {"tool": "tool_name", "parameters": {...}}')
        lines.append("")
        lines.append("‚úÖ CORRECT EXAMPLES (note the 'TOOL_CALL:' prefix):")
        lines.append('TOOL_CALL: {"tool": "query_rag", "parameters": {"query": "diagnostic criteria for condition", "purpose": "need classification thresholds"}}')
        lines.append('TOOL_CALL: {"tool": "call_calculate_bmi", "parameters": {"weight_kg": 70, "height_m": 1.75}}')
        lines.append('TOOL_CALL: {"tool": "query_extras", "parameters": {"keywords": ["assessment", "criteria", "classification", "guidelines"]}}')
        lines.append("")
        lines.append("‚ùå WRONG (missing 'TOOL_CALL:' prefix):")
        lines.append('{"tool": "query_rag", ...}  ‚Üê MISSING TOOL_CALL: prefix!')
        lines.append("")
        lines.append("‚ùå WRONG (missing closing brace):")
        lines.append('TOOL_CALL: {"tool": "query_rag", "parameters": {"query": "test"}  ‚Üê MISSING }')
        lines.append("")
        lines.append("üí° ITERATIVE WORKFLOW:")
        lines.append("ROUND 1 - Initial Analysis:")
        lines.append("  ‚Ä¢ Analyze task prompt + input text")
        lines.append("  ‚Ä¢ Identify key metrics/measurements/entities/information")
        lines.append("  ‚Ä¢ Call functions (calculations/transformations) + query_extras (hints)")
        lines.append("")
        lines.append("ROUND 2 - Build Context:")
        lines.append("  ‚Ä¢ Review function/extras results - REMEMBER these values!")
        lines.append("  ‚Ä¢ Build RAG keywords from what you learned")
        lines.append("  ‚Ä¢ Call query_rag (fetch guidelines/standards/criteria)")
        lines.append("")
        lines.append("ROUND 3+ - Assess & Fill Gaps:")
        lines.append("  ‚Ä¢ Assess: Do I have ALL info needed?")
        lines.append("  ‚Ä¢ If NO: Identify gaps ‚Üí Call NEW tools (not duplicates!) ‚Üí Reassess")
        lines.append("  ‚Ä¢ If YES: Output final JSON using ALL tool results")
        lines.append("")
        lines.append("=" * 80)
        lines.append("üî¥ CRITICAL REQUIREMENTS:")
        lines.append("=" * 80)
        lines.append("")
        lines.append("1. USE EVERY TOOL RESULT:")
        lines.append("   ‚Ä¢ Function results ‚Üí Include exact values in JSON output")
        lines.append("   ‚Ä¢ RAG documents ‚Üí Cite and apply criteria/guidelines")
        lines.append("   ‚Ä¢ Extras hints ‚Üí Apply patterns and guidance")
        lines.append("")
        lines.append("2. NO DUPLICATE CALCULATIONS:")
        lines.append("   ‚Ä¢ Track what you've calculated")
        lines.append("   ‚Ä¢ DO NOT call same function with same parameters twice")
        lines.append("   ‚Ä¢ Remember results from previous tool calls")
        lines.append("")
        lines.append("3. INCORPORATE ALL RETRIEVED INFO:")
        lines.append("   ‚Ä¢ Reference RAG sources in your reasoning")
        lines.append("   ‚Ä¢ Apply extras guidance to your extraction")
        lines.append("   ‚Ä¢ Include ALL calculated values in final JSON")
        lines.append("=" * 80 + "\n")

        # Add conversation history with windowing
        # PERFORMANCE: Apply windowing to preserve tool results while reducing context
        windowed_history = self._apply_conversation_window(self.context.conversation_history)

        for msg in windowed_history:
            if msg.role == 'system':
                # Already added above with tools
                continue
            elif msg.role == 'user':
                lines.append(f"\n{'=' * 80}")
                lines.append("USER REQUEST:")
                lines.append(f"{'=' * 80}")
                lines.append(msg.content)
            elif msg.role == 'assistant':
                lines.append(f"\nASSISTANT: {msg.content}")
            elif msg.role == 'tool':
                lines.append(f"\n{'=' * 40}")
                lines.append(f"TOOL RESULT ({msg.name}):")
                lines.append(f"{'=' * 40}")
                lines.append(msg.content)

        return "\n".join(lines)

    def _parse_text_response_for_tools(self, text: str) -> Dict[str, Any]:
        """
        Parse text response for tool calls or JSON (fallback for non-tool-calling providers)

        ENHANCED: Detects TOOL_CALL requests in text format with proper nested JSON handling
        """
        tool_calls = []

        # FIXED: Better pattern that handles nested JSON braces
        # Look for TOOL_CALL: followed by balanced braces
        tool_call_lines = re.findall(r'TOOL_CALL:\s*(.+?)(?=\n|$)', text, re.IGNORECASE | re.MULTILINE)

        if tool_call_lines:
            # Parse tool calls
            for i, match_text in enumerate(tool_call_lines):
                match_text = match_text.strip()

                # Try to extract complete JSON (handles nested braces)
                try:
                    # Find the complete JSON object by counting braces
                    json_str = self._extract_complete_json(match_text)

                    if not json_str:
                        logger.info(f"‚ÑπÔ∏è Skipping incomplete tool call JSON (LLM response may have been truncated): {match_text[:100]}...")
                        logger.info("   ‚Üí Continuing with other valid tool calls (this is handled gracefully)")
                        continue

                    tool_request = json.loads(json_str)
                    tool_name = tool_request.get('tool', '')
                    parameters = tool_request.get('parameters', {})

                    if not tool_name:
                        logger.warning(f"Tool call missing 'tool' field: {json_str}")
                        continue

                    tool_calls.append({
                        'id': f"call_{int(time.time() * 1000)}_{i}",
                        'type': 'function',
                        'function': {
                            'name': tool_name,
                            'arguments': json.dumps(parameters)
                        }
                    })
                    logger.info(f"‚úì Parsed tool call: {tool_name}")

                except json.JSONDecodeError as e:
                    logger.info(f"‚ÑπÔ∏è Skipping malformed tool call JSON: {match_text[:100]}... | Error: {e}")
                    logger.info("   ‚Üí Continuing with other valid tool calls")
                except Exception as e:
                    logger.info(f"‚ÑπÔ∏è Skipping unparseable tool call: {match_text[:100]}... | Error: {e}")
                    logger.info("   ‚Üí Continuing with other valid tool calls")

            if tool_calls:
                logger.info(f"‚úÖ Successfully parsed {len(tool_calls)} tool calls from text response")
                return {
                    'content': text,
                    'tool_calls': tool_calls,
                    'finish_reason': 'tool_calls'
                }

        # Check if it looks like final JSON output
        if '{' in text and '}' in text:
            parsed = self._try_parse_json(text)
            if parsed:
                logger.info("Detected final JSON output")
                return {'content': text, 'tool_calls': [], 'finish_reason': 'stop'}

        # Otherwise, assume it's analysis/thinking
        logger.info("LLM is thinking/analyzing")
        return {'content': text, 'tool_calls': [], 'finish_reason': 'continue'}

    def _extract_complete_json(self, text: str) -> Optional[str]:
        """
        Extract complete JSON object from text by counting balanced braces

        Handles nested JSON like: {"tool": "x", "parameters": {"a": "b", "c": "d"}}
        """
        text = text.strip()
        if not text.startswith('{'):
            return None

        brace_count = 0
        in_string = False
        escape_next = False

        for i, char in enumerate(text):
            if escape_next:
                escape_next = False
                continue

            if char == '\\':
                escape_next = True
                continue

            if char == '"' and not escape_next:
                in_string = not in_string
                continue

            if in_string:
                continue

            if char == '{':
                brace_count += 1
            elif char == '}':
                brace_count -= 1

            # Found complete JSON object
            if brace_count == 0:
                return text[:i+1]

        # Incomplete JSON
        return None

    def _build_extraction_result(self) -> Dict[str, Any]:
        """Build final extraction result with detailed tracking"""
        # Count tool usage by type
        extras_count = 0
        rag_count = 0
        function_count = 0
        extras_details = []
        rag_details = []
        function_details = []

        for result in self.context.tool_results:
            if result.type == 'extras':
                extras_count += 1
                if result.success and result.result:
                    for extra in result.result:
                        extras_details.append({
                            'id': extra.get('id', 'N/A'),
                            'type': extra.get('type', 'unknown'),
                            'content': extra.get('content', ''),
                            'relevance_score': extra.get('relevance_score', 0),
                            'matched_keywords': extra.get('matched_keywords', [])
                        })
            elif result.type == 'rag':
                rag_count += 1
                if result.success and result.result:
                    for chunk in result.result:
                        rag_details.append({
                            'content': chunk.get('content', '') or chunk.get('text', ''),
                            'source': chunk.get('metadata', {}).get('source', 'unknown') if isinstance(chunk.get('metadata'), dict) else 'unknown',
                            'score': chunk.get('score', 0)
                        })
            elif result.type == 'function':
                function_count += 1
                if result.success:
                    # Extract function name from tool call history
                    func_name = 'unknown'
                    for msg in self.context.conversation_history:
                        if msg.role == 'assistant' and msg.tool_calls:
                            for tc in msg.tool_calls:
                                if tc.id == result.tool_call_id:
                                    func_name = tc.name.replace('call_', '')
                                    break

                    function_details.append({
                        'name': func_name,
                        'result': result.result,
                        'message': result.message or ''
                    })

        logger.info(f"üìä TOOL USAGE SUMMARY: Extras={extras_count}, RAG={rag_count}, Functions={function_count}")

        return {
            'original_clinical_text': self.context.original_text,
            'clinical_text': self.context.clinical_text,
            'redacted_text': self.context.redacted_text,
            'normalized_text': self.context.normalized_text,
            'input_label_value': None,
            'label_context': self.context.label_context,
            'stage3_output': self.context.final_output or {},
            'stage4_final_output': self.context.final_output or {},

            # Usage counts for playground display (FIXED)
            'extras_used': extras_count,
            'rag_used': rag_count,
            'functions_called': function_count,

            'agentic_metadata': {
                'version': '1.0.0',
                'execution_mode': 'agentic_async',
                'iterations': self.context.iteration,
                'total_tool_calls': self.context.total_tool_calls,
                'final_state': self.context.state.value,
                'tool_results_count': len(self.context.tool_results),
                'conversation_length': len(self.context.conversation_history),
                'stall_counter': self.context.stall_counter,
                'consecutive_no_progress': self.context.consecutive_no_progress
            },
            'processing_metadata': {
                'had_label': self.context.label_context is not None,
                'final_state': self.context.state.value,
                'agentic_execution': True,

                # Detailed tracking for playground display (FIXED)
                'extras_details': extras_details,
                'rag_details': rag_details,
                'function_calls_details': function_details,

                # Tool results summary
                'tool_results_summary': {
                    'extras': extras_count,
                    'rag': rag_count,
                    'functions': function_count
                }
            }
        }

    def _build_failure_result(self, error_message: str) -> Dict[str, Any]:
        """Build failure result"""
        return {
            'original_clinical_text': self.context.original_text if self.context else '',
            'clinical_text': self.context.clinical_text if self.context else '',
            'error': error_message,
            'stage3_output': {},
            'stage4_final_output': {},
            'agentic_metadata': {
                'version': '1.0.0',
                'execution_mode': 'agentic_async',
                'iterations': self.context.iteration if self.context else 0,
                'total_tool_calls': self.context.total_tool_calls if self.context else 0,
                'final_state': 'failed',
                'error': error_message
            },
            'processing_metadata': {
                'had_label': False,
                'final_state': 'failed',
                'agentic_execution': True
            }
        }

    def _get_label_context_string(self, label_value: Any) -> Optional[str]:
        """Get label context from mapping"""
        if label_value is None:
            return None

        label_mapping = self.app_state.data_config.label_mapping
        label_key = str(label_value)

        if label_key in label_mapping:
            return label_mapping[label_key]

        try:
            numeric_value = int(label_value)
            if numeric_value in label_mapping:
                return label_mapping[numeric_value]
        except (ValueError, TypeError):
            pass

        return None

    def _preprocess_clinical_text(self, text: str) -> str:
        """Preprocess clinical text"""
        if not text or not text.strip():
            return ""

        processed_text = text

        # Pattern normalization
        if self.app_state.data_config.enable_pattern_normalization and self.regex_preprocessor:
            try:
                normalized = self.regex_preprocessor.preprocess(processed_text)
                if self.app_state.data_config.save_normalized_text and self.context:
                    self.context.normalized_text = normalized
                processed_text = normalized
                logger.info("Pattern normalization applied")
            except Exception as e:
                logger.warning(f"Regex preprocessing failed: {e}")

        # PHI redaction
        if self.app_state.data_config.enable_phi_redaction:
            try:
                from core.pii_redactor import create_redactor

                redactor = create_redactor(
                    entity_types=self.app_state.data_config.phi_entity_types,
                    method=self.app_state.data_config.redaction_method
                )

                redacted, redactions = redactor.redact(processed_text)
                if self.app_state.data_config.save_redacted_text and self.context:
                    self.context.redacted_text = redacted
                processed_text = redacted
                logger.info(f"PHI redaction applied: {len(redactions)} entities")
            except Exception as e:
                logger.warning(f"PHI redaction failed: {e}")

        return processed_text.strip()
