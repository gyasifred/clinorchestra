# ğŸ—ï¸ COMPLETE ARCHITECTURE ANALYSIS

## Overview

This platform is a **universal, autonomous clinical data extraction system** that uses LLM-powered agents to orchestrate multiple knowledge sources and computational tools. It adapts to ANY clinical task through prompts and JSON schemas, requiring no code changes for new use cases.

**Version:** 1.0.0
**Architecture Type:** Multi-Agent, Multi-Modal Knowledge System

---

## ğŸ“Š HIGH-LEVEL ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INPUTS CLINICAL TASK                        â”‚
â”‚  (Via UI or API: Text + Task Definition + JSON Schema + Labels)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INTELLIGENT ORCHESTRATION                       â”‚
â”‚         (STRUCTURED Mode OR ADAPTIVE Mode - Both Autonomous)        â”‚
â”‚                                                                      â”‚
â”‚  The LLM analyzes the task and orchestrates these components:       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG    â”‚      â”‚ FUNCTION â”‚     â”‚  EXTRAS  â”‚
    â”‚  ENGINE  â”‚      â”‚ REGISTRY â”‚     â”‚ MANAGER  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  JSON EXTRACTOR â”‚
                    â”‚   & VALIDATOR   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  STRUCTURED OUTPUT   â”‚
                  â”‚  (Task-specific JSON)â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ CORE COMPONENTS

### 1. AGENT SYSTEM (The Orchestrators)

The agent system provides two execution modes, both autonomous and universal:

#### A. ExtractionAgent (STRUCTURED Mode)
**File:** `core/agent_system.py`

**Purpose:** Systematic, predictable 4-stage extraction for production workflows

**Execution Flow:**
```
Stage 1: TASK ANALYSIS
â”œâ”€ LLM reads: Clinical text, task definition, JSON schema, labels (optional)
â”œâ”€ LLM analyzes: What information is needed?
â”œâ”€ LLM decides: Which tools to call? (RAG, Functions, Extras)
â””â”€ Output: Tool request list

Stage 2: TOOL EXECUTION (ASYNC - Parallel!)
â”œâ”€ Execute RAG queries (knowledge retrieval)
â”œâ”€ Execute Functions (calculations, conversions)
â”œâ”€ Execute Extras queries (supplementary hints)
â””â”€ All run in PARALLEL for 60-75% speedup

Stage 3: EXTRACTION
â”œâ”€ LLM receives: Original text + All tool results
â”œâ”€ LLM extracts: Information matching JSON schema
â”œâ”€ LLM uses: Tool results to inform extraction
â””â”€ Output: Structured JSON

Stage 4: RAG REFINEMENT (Optional)
â”œâ”€ If RAG was used, refine extraction
â”œâ”€ Validate against retrieved knowledge
â””â”€ Output: Final validated JSON
```

**Key Features:**
- âœ… Predictable, systematic workflow
- âœ… Autonomous tool selection (LLM decides what to call)
- âœ… ASYNC parallel tool execution
- âœ… Works for ANY clinical task
- âœ… Label context is OPTIONAL

---

#### B. AgenticAgent (ADAPTIVE Mode)
**File:** `core/agentic_agent.py`

**Purpose:** Continuous iteration for evolving, complex tasks requiring dynamic adaptation

**Execution Flow:**
```
Continuous Loop:
â”œâ”€ Iteration 1: LLM analyzes â†’ Calls tools â†’ Gets results
â”œâ”€ Iteration 2: LLM learns from results â†’ Calls MORE tools â†’ Gets results
â”œâ”€ Iteration 3: LLM refines understanding â†’ Calls tools â†’ Gets results
â”œâ”€ ...continues until extraction complete or max iterations
â””â”€ PAUSE/RESUME states for dynamic control

Key Difference from STRUCTURED:
- Can call tools MULTIPLE times with different queries
- Learns and adapts strategy based on results
- "That BMI result tells me I need growth percentile next"
- More flexible but less predictable
```

**Key Features:**
- âœ… Continuous iterative refinement
- âœ… Dynamic tool calling (multiple iterations)
- âœ… Learns from tool results
- âœ… ASYNC parallel tool execution
- âœ… PAUSE/RESUME states
- âœ… Works for ANY clinical task
- âœ… Label context is OPTIONAL

**Native Tool Calling:**
- Uses OpenAI/Anthropic function calling API
- LLM natively requests tool calls
- Maintains conversation context across iterations

---

#### C. AgentFactory
**File:** `core/agent_factory.py`

**Purpose:** Creates the appropriate agent based on user's execution mode choice

**Logic:**
```python
if user_selects_adaptive_mode:
    return AdaptiveAgent(...)  # For evolving tasks
else:
    return ExtractionAgent(...)  # For predictable workflows
```

**Important:** Both agents are equally capable - they differ only in execution style, not capability!

---

### 2. LLM MANAGER (The Brain)

**File:** `core/llm_manager.py`

**Purpose:** Manages communication with ANY LLM provider through unified interface

**Supported Providers:**
```
â”œâ”€ OpenAI (GPT-4, GPT-4 Turbo, GPT-3.5)
â”œâ”€ Anthropic (Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku)
â”œâ”€ Google (Gemini models)
â”œâ”€ Azure OpenAI (all Azure-hosted OpenAI models)
â”œâ”€ Local models (via Unsloth - quantized LLaMA, Mistral, etc.)
â””â”€ Future: Any provider with OpenAI-compatible interface
```

**Key Features:**
- **Unified Interface:** Same code works across all providers
- **Native Tool Calling:** Supports function calling API
- **Streaming:** Real-time response streaming
- **Token Management:** Automatic token counting and limits
- **Retry Logic:** Exponential backoff on failures
- **Rate Limiting:** Respects provider rate limits
- **Error Handling:** Graceful degradation

**Why It Matters:**
Users can switch LLM providers without changing any code, enabling:
- Cost optimization (use cheaper models for simple tasks)
- Performance tuning (use powerful models for complex tasks)
- Provider redundancy (fallback if one provider is down)

---

### 3. RAG ENGINE (Knowledge Retrieval)

**File:** `core/rag_engine.py`

**Purpose:** Semantic search over medical documents to retrieve relevant knowledge

**Architecture:**
```
Indexing Phase:
â”œâ”€ User uploads documents (PDFs, text files, markdown)
â”œâ”€ System chunks documents into semantic segments
â”œâ”€ Generates embeddings using transformer models:
â”‚   â”œâ”€ sentence-transformers/all-MiniLM-L6-v2 (fast, lightweight)
â”‚   â””â”€ BAAI/bge-large-en-v1.5 (high quality, slower)
â”œâ”€ Stores in vector database (FAISS)
â””â”€ Persists to disk for reuse

Query Phase:
â”œâ”€ Agent formulates query (e.g., "normal BMI ranges for children")
â”œâ”€ RAG engine converts query to embedding
â”œâ”€ Searches vector DB for most similar chunks
â”œâ”€ Returns top-k most relevant passages with similarity scores
â””â”€ Agent uses retrieved knowledge to inform extraction
```

**Use Cases:**
- Query growth charts for pediatric assessments
- Retrieve medication dosing guidelines
- Find disease classification criteria
- Look up normal lab value ranges
- Access clinical practice guidelines

**Key Features:**
- âœ… **Semantic Search:** Understands meaning, not just keywords
- âœ… **Configurable Models:** Choose speed vs quality
- âœ… **Persistent Storage:** Index once, query many times
- âœ… **Fast Retrieval:** Optimized vector search
- âœ… **Universal:** Works with ANY medical domain documents
- âœ… **Batch Processing:** Index multiple documents efficiently

**Technical Details:**
- Vector database: FAISS (Facebook AI Similarity Search - fast, local, persistent)
- Embedding dimensions: 384 (MiniLM) or 1024 (BGE)
- Chunking strategy: Semantic segmentation with overlap
- Similarity metric: Cosine similarity

---

### 4. FUNCTION REGISTRY (Computational Tools)

**File:** `core/function_registry.py`

**Purpose:** Provides calculators and data processing functions that agents can call

**Architecture:**
```
Function Definition (JSON format):
{
    "name": "calculate_bmi",
    "description": "Calculate Body Mass Index from weight and height",
    "parameters": {
        "weight_kg": {
            "type": "float",
            "description": "Weight in kilograms"
        },
        "height_m": {
            "type": "float",
            "description": "Height in meters"
        }
    },
    "code": "def calculate_bmi(weight_kg, height_m): ..."
}

Registration Process:
â”œâ”€ Functions stored in /functions/*.json
â”œâ”€ Registry loads all functions on startup
â”œâ”€ Validates function signatures and parameters
â”œâ”€ Makes them available to agents via tool calling
â””â”€ Handles type conversion (stringâ†’number, etc.)

Execution Flow:
â”œâ”€ Agent requests: calculate_bmi(weight_kg=70, height_m=1.75)
â”œâ”€ Registry validates parameters
â”œâ”€ Registry executes function in safe environment
â”œâ”€ Returns result: {"bmi": 22.86, "category": "Normal"}
â””â”€ Agent uses result in extraction process
```

**Built-in Functions:**
- `calculate_bmi()` - Body Mass Index calculation
- `calculate_weight_change_percent()` - Weight change over time
- `calculate_growth_percentile()` - CDC growth percentiles
- `calculate_z_score()` - Standard deviation scores
- `convert_units()` - Unit conversions (kgâ†”lbs, cmâ†”in, etc.)
- Custom functions can be added by dropping JSON files!

**Key Features:**
- âœ… **Dynamic Loading:** Add functions without code changes
- âœ… **Type Conversion:** Handles stringâ†’number, etc. automatically
- âœ… **Safe Execution:** Sandboxed environment
- âœ… **Parameter Validation:** Ensures correct inputs
- âœ… **Error Handling:** Graceful failure with informative messages
- âœ… **Extensible:** Easy to add domain-specific calculators

**Extension Example:**
To add a new function, simply create `/functions/my_function.json`:
```json
{
    "name": "calculate_egfr",
    "description": "Calculate estimated glomerular filtration rate",
    "parameters": {
        "creatinine": {"type": "float", "description": "Serum creatinine mg/dL"},
        "age": {"type": "int", "description": "Patient age in years"},
        "sex": {"type": "string", "description": "M or F"}
    },
    "code": "def calculate_egfr(creatinine, age, sex): ..."
}
```

---

### 5. EXTRAS MANAGER (Supplementary Hints)

**File:** `core/extras_manager.py`

**Purpose:** Provides task-specific hints, guidelines, and tips to help the LLM make accurate extractions

**Architecture:**
```
Extra Definition (JSON format):
{
    "id": "unique_identifier",
    "type": "guideline" | "reference" | "hint" | "conversion",
    "keywords": ["malnutrition", "z-score", "percentile"],
    "content": "CRITICAL: Lower percentiles correspond to NEGATIVE z-scores.
                Common conversions: 3rd percentile â‰ˆ z-score -1.88...",
    "metadata": {
        "category": "nutrition",
        "priority": "high",
        "domain": "pediatrics"
    }
}

Matching Process:
â”œâ”€ Agent provides keywords based on task (e.g., ["malnutrition", "pediatric"])
â”œâ”€ Extras Manager performs fuzzy matching against all extras
â”œâ”€ Ranks extras by relevance score
â”œâ”€ Returns top matches with content
â””â”€ Agent receives context-specific guidance

Example Extras:
â”œâ”€ "Z-score and percentile relationship for growth assessment"
â”œâ”€ "Common malnutrition indicators and diagnostic thresholds"
â”œâ”€ "Unit conversion reference for medication dosing"
â”œâ”€ "Clinical interpretation guidelines for laboratory values"
â””â”€ "Disease classification criteria (ICD-10, DSM-5, etc.)"
```

**Why This Matters:**
- Reduces LLM hallucinations with domain-specific knowledge
- Provides critical context (e.g., "lower percentile = negative z-score")
- Helps avoid common clinical interpretation errors
- Works universally across domains (not limited to malnutrition!)

**Key Features:**
- âœ… **Keyword-based Matching:** Flexible query system
- âœ… **Fuzzy Search:** Typo-tolerant matching
- âœ… **Relevance Ranking:** Best matches returned first
- âœ… **Easy Extension:** Add extras by dropping JSON files
- âœ… **Universal:** Works across all clinical domains
- âœ… **Priority System:** High-priority extras surfaced first

**Storage:** `/extras/*.json`

---

### 6. PREPROCESSING SYSTEM

#### A. Regex Preprocessor
**File:** `core/regex_preprocessor.py`

**Purpose:** Normalizes clinical text before LLM processing to improve accuracy

**Transformations:**
```
Date Normalization:
â”œâ”€ "12/25/2023" â†’ "2023-12-25"
â”œâ”€ "Dec 25, 2023" â†’ "2023-12-25"
â””â”€ "25-Dec-2023" â†’ "2023-12-25"

Number Extraction:
â”œâ”€ "weight: 25.5 kg" â†’ captures "25.5"
â”œâ”€ "BP 120/80" â†’ captures "120", "80"
â””â”€ "5'10\"" â†’ converts to "177.8 cm"

Unit Standardization:
â”œâ”€ "5 feet 10 inches" â†’ "177.8 cm"
â”œâ”€ "150 lbs" â†’ "68.04 kg"
â””â”€ "98.6Â°F" â†’ "37Â°C"

Range Parsing:
â”œâ”€ "BP 120-80" â†’ "systolic: 120, diastolic: 80"
â”œâ”€ "Weight 50-55 kg" â†’ "weight_min: 50, weight_max: 55"
â””â”€ "Age 5-10 years" â†’ "age_min: 5, age_max: 10"
```

**Configuration:**
- Patterns stored in JSON format
- Loaded at startup
- Applied sequentially
- User-customizable per task

**Key Features:**
- âœ… **Customizable Patterns:** Define task-specific rules
- âœ… **Sequential Application:** Order matters for complex transforms
- âœ… **Original Preservation:** Keeps original text for reference
- âœ… **Improves Accuracy:** Standardized format helps LLM

---

#### B. PII Redactor
**File:** `core/pii_redactor.py`

**Purpose:** Removes or masks sensitive information for HIPAA compliance

**Entity Detection:**
```
Supported Entities:
â”œâ”€ PERSON (names)
â”œâ”€ DATE (dates of birth, visit dates)
â”œâ”€ MRN (medical record numbers)
â”œâ”€ ORGANIZATION (hospital names, clinics)
â”œâ”€ GPE (cities, states)
â”œâ”€ LOC (specific locations)
â””â”€ Custom entity types (extensible)
```

**Redaction Modes:**
```
MASK Mode:
â”œâ”€ "John Doe" â†’ "[PERSON]"
â”œâ”€ "12/25/2023" â†’ "[DATE]"
â””â”€ "MRN: 12345" â†’ "MRN: [MRN]"

REMOVE Mode:
â”œâ”€ "John Doe visited" â†’ "visited"
â”œâ”€ Completely removes entity
â””â”€ Preserves sentence structure

HASH Mode:
â”œâ”€ "John Doe" â†’ "PERSON_abc123"
â”œâ”€ Consistent hashing (same person = same hash)
â””â”€ Enables re-identification if needed
```

**Key Features:**
- âœ… **NER-based Detection:** Uses spaCy for accuracy
- âœ… **Multiple Strategies:** Choose redaction approach
- âœ… **Configurable Entities:** Select which to redact
- âœ… **HIPAA Compliance:** Support for de-identification
- âœ… **Reversible Hashing:** Optional re-identification

---

### 7. JSON PROCESSING

#### A. JSON Parser
**File:** `core/json_parser.py`

**Purpose:** Extracts and validates JSON from LLM responses (handles LLM quirks)

**Parsing Strategies (Applied Sequentially):**
```
1. Clean JSON Extraction:
   â”œâ”€ Finds JSON blocks in markdown (```json ... ```)
   â”œâ”€ Handles nested structures
   â”œâ”€ Validates against schema
   â””â”€ Returns if valid

2. Fuzzy JSON Extraction:
   â”œâ”€ Handles malformed JSON (missing quotes, trailing commas)
   â”œâ”€ Fixes common LLM errors
   â”œâ”€ Attempts repair and re-validation
   â””â”€ Returns if repairable

3. Fallback Parsing:
   â”œâ”€ Extracts partial JSON
   â”œâ”€ Returns best-effort structure
   â”œâ”€ Logs which fields are missing
   â””â”€ Allows graceful degradation
```

**Key Features:**
- âœ… **Robust:** Handles LLM inconsistencies
- âœ… **Multiple Strategies:** Fallback chain ensures success
- âœ… **Schema Validation:** Ensures output matches expected format
- âœ… **Detailed Logging:** Tracks which parsing method succeeded
- âœ… **Error Recovery:** Attempts repair before failing

---

#### B. Output Handler
**File:** `core/output_handler.py`

**Purpose:** Formats and exports extraction results in multiple formats

**Output Formats:**
```
CSV Export:
â”œâ”€ Flattened JSON for spreadsheet compatibility
â”œâ”€ Nested fields â†’ dot notation (e.g., "patient.age")
â”œâ”€ Arrays â†’ comma-separated values
â””â”€ Metadata columns (timestamp, model_used, etc.)

JSON Export:
â”œâ”€ Full structured output preserved
â”œâ”€ Pretty-printed for readability
â”œâ”€ Includes extraction metadata
â””â”€ Per-record or batch export

Excel Export:
â”œâ”€ Multi-sheet workbooks
â”œâ”€ Sheet 1: Extracted data
â”œâ”€ Sheet 2: Metadata
â”œâ”€ Sheet 3: Processing logs
â””â”€ Formatted for analysis

Future:
â””â”€ Database insertion (SQL, MongoDB, etc.)
```

**Features:**
- **Schema Flattening:** Nested â†’ flat for CSV
- **Metadata Inclusion:** Timestamps, model info, tool usage
- **Batch Processing:** Efficient bulk export
- **Incremental Saving:** Save as processing completes

---

### 8. PROMPT MANAGEMENT

**File:** `core/prompt_templates.py`

**Purpose:** Manages all prompt templates and dynamic prompt generation

**Template Types:**
```
DEFAULT Templates (Generic):
â”œâ”€ Task-agnostic base templates
â”œâ”€ Work for any clinical domain
â”œâ”€ Schema-driven prompt generation
â””â”€ No hardcoded task assumptions

EXAMPLE Templates (Illustrative):
â”œâ”€ MALNUTRITION_* (pre-configured for malnutrition)
â”œâ”€ DIABETES_* (pre-configured for diabetes)
â”œâ”€ NOT required - just examples!
â””â”€ Users can define custom templates

Custom Templates:
â”œâ”€ User-defined task-specific prompts
â”œâ”€ Variable substitution ({task}, {schema}, etc.)
â”œâ”€ Template inheritance
â””â”€ Easy customization
```

**Dynamic Prompt Building:**
```
Prompt Generation Process:
â”œâ”€ 1. Load base template (or use default)
â”œâ”€ 2. Inject user's task definition
â”œâ”€ 3. Format JSON schema as instructions
â”œâ”€ 4. Include label context (if provided - OPTIONAL)
â”œâ”€ 5. Format tool results (if Stage 2 complete)
â”œâ”€ 6. Add examples (if configured)
â”œâ”€ 7. Adapt for execution mode (STRUCTURED vs ADAPTIVE)
â””â”€ 8. Return complete prompt for LLM
```

**Key Features:**
- âœ… **Template Inheritance:** Build on base templates
- âœ… **Dynamic Schema Integration:** Schema â†’ instructions
- âœ… **Variable Substitution:** Flexible customization
- âœ… **Mode-specific Prompts:** Different for STRUCTURED vs ADAPTIVE
- âœ… **Optional Labels:** Works with or without label context
- âœ… **Easy Customization:** Users can edit/create templates

---

### 9. CONFIGURATION SYSTEM

#### A. Model Configuration
**File:** `core/model_config.py`

**Purpose:** Manages LLM provider settings and parameters

**Configuration Options:**
```python
ModelConfig:
â”œâ”€ provider: "openai" | "anthropic" | "azure" | "local"
â”œâ”€ model_name: "gpt-4" | "claude-3-5-sonnet-20241022" | ...
â”œâ”€ api_key: str (user's API key)
â”œâ”€ temperature: float (0.0 - 2.0, controls randomness)
â”œâ”€ max_tokens: int (response length limit)
â”œâ”€ top_p: float (nucleus sampling)
â”œâ”€ frequency_penalty: float (reduce repetition)
â”œâ”€ presence_penalty: float (encourage diversity)
â””â”€ Provider-specific:
    â”œâ”€ endpoint: str (custom API endpoint)
    â”œâ”€ api_version: str (API version for Azure)
    â”œâ”€ deployment_name: str (Azure deployment)
    â””â”€ base_url: str (for local models)
```

---

#### B. App State
**File:** `core/app_state.py`

**Purpose:** Central state management for the entire application

**Managed State:**
```
Configuration Objects:
â”œâ”€ model_config: LLM settings
â”œâ”€ prompt_config: Prompt templates and settings
â”œâ”€ rag_config: RAG indexing and query settings
â”œâ”€ processing_config: Batch processing parameters
â”œâ”€ adaptive_config: ADAPTIVE mode settings (max_iterations, max_tool_calls)
â””â”€ data_config: Input data column mappings, label mappings

Initialized Components:
â”œâ”€ llm_manager: Initialized LLM client
â”œâ”€ rag_engine: Initialized RAG engine with vector DB
â”œâ”€ function_registry: Loaded function definitions
â”œâ”€ extras_manager: Loaded extras
â””â”€ Current processing status and logs
```

**Key Features:**
- âœ… **Centralized:** Single source of truth
- âœ… **Lazy Initialization:** Components created on demand
- âœ… **Configuration Validation:** Ensures valid settings
- âœ… **State Persistence:** Saves to disk
- âœ… **Easy Access:** All components accessible globally

---

#### C. Config Persistence
**File:** `core/config_persistence.py`

**Purpose:** Saves/loads configurations to/from disk

**Features:**
- Automatic saving to JSON files
- Load on application startup
- Version migration support
- Backup creation before overwrites
- Handles nested configuration objects

**Storage Location:** `~/.clinannotate/config.json`

---

### 10. UI SYSTEM (Gradio Interface)

**Directory:** `ui/`

The UI provides a user-friendly interface for all platform capabilities through **9 comprehensive tabs**:

**Complete Tab List:**
1. **Model Configuration** - LLM provider and execution mode setup
2. **Data Configuration** - Input data upload and column mapping
3. **Prompt Configuration** - Task definition and JSON schema
4. **RAG** - Document upload and vector database management
5. **Regex Patterns** - Text preprocessing rules (120+ built-in patterns)
6. **Extras (Hints)** - Task-specific knowledge (183+ built-in hints)
7. **Custom Functions** - Computational tools (40+ built-in calculators)
8. **Playground** - Single-text testing and debugging
9. **Processing** - Batch execution and monitoring

All tabs support **YAML and JSON** file loading for easy configuration sharing!

---

#### A. Config Tab (`ui/config_tab.py`)
**Purpose:** LLM and execution mode configuration

**Features:**
- LLM provider selection (OpenAI, Anthropic, Azure, Local)
- API key management (secure input)
- Model selection (GPT-4, Claude 3.5, etc.)
- Execution mode selection:
  - âœ… STRUCTURED Mode (for predictable workflows)
  - âœ… ADAPTIVE Mode (for evolving tasks)
- Model parameter tuning (temperature, max_tokens, etc.)
- ADAPTIVE mode settings (max iterations, max tool calls)
- Configuration save/load
- Connection testing

---

#### B. Data Tab (`ui/data_tab.py`)
**Purpose:** Data input and mapping configuration

**Features:**
- **File Upload:** CSV, Excel, JSON formats
- **Column Mapping:**
  - Select text column (clinical notes)
  - Select label column (OPTIONAL - for supervised tasks)
  - Select ID column (optional)
- **Label Mapping:**
  - Map label values to descriptive text
  - Example: 0 â†’ "No malnutrition", 1 â†’ "Malnutrition present"
  - COMPLETELY OPTIONAL - system works without labels
- **Data Preview:** View uploaded data
- **Validation:** Checks for required columns

**Important:** Label context is OPTIONAL. The system adapts to:
- Supervised tasks (with labels)
- Unsupervised tasks (without labels)
- Every task can have different requirements

---

#### C. Prompt Tab (`ui/prompt_tab.py`)
**Purpose:** Prompt template and task definition management

**Features:**
- **Prompt Template Selection:**
  - DEFAULT (generic, task-agnostic)
  - MALNUTRITION (example template)
  - DIABETES (example template)
  - CUSTOM (user-defined)
- **Custom Prompt Editing:**
  - Full text editor for prompt customization
  - Variable substitution support
  - Real-time preview
- **JSON Schema Definition:**
  - Define expected output structure
  - Field types, descriptions, requirements
  - Nested object support
- **Template Testing:**
  - Test prompts with sample text
  - Preview generated prompts
- **Task Description:**
  - Free-text task definition
  - Instructions for the LLM

---

#### D. RAG Tab (`ui/rag_tab.py`)
**Purpose:** Document upload and RAG management

**Features:**
- **Document Upload:**
  - PDF, TXT, Markdown support
  - Batch upload multiple documents
  - Preview uploaded documents
- **Vector Database Management:**
  - View indexed documents
  - Delete/update documents
  - Rebuild index
- **Embedding Model Selection:**
  - sentence-transformers/all-MiniLM-L6-v2 (fast)
  - BAAI/bge-large-en-v1.5 (high quality)
- **Index Building:**
  - Progress tracking
  - Chunking configuration
  - Status monitoring
- **Query Testing:**
  - Test RAG queries
  - View retrieved passages
  - Check relevance scores

---

#### E. Regex Patterns Tab (`ui/patterns_tab.py`)
**Purpose:** Text preprocessing pattern management

**Features:**
- **Pattern Registration:**
  - Define regex patterns for text normalization
  - Set replacement rules
  - Enable/disable patterns individually
- **File Upload:**
  - Upload pattern files (YAML/JSON)
  - Batch load multiple patterns
- **Pattern Testing:**
  - Test patterns against sample text
  - Preview before/after transformations
  - Validate regex syntax
- **Built-in Patterns:**
  - Standardize medical units (mg, kg, etc.)
  - Normalize blood pressure formats
  - Fix date formats
  - Remove extra whitespace
  - And 100+ more medical text patterns!
- **Preview/Edit:**
  - View all registered patterns in dataframe
  - Edit pattern details (name, regex, replacement)
  - Remove unwanted patterns
  - Toggle patterns on/off

**Use Case:** Standardize inconsistent clinical text BEFORE it goes to the LLM

---

#### F. Extras (Hints) Tab (`ui/extras_tab.py`)
**Purpose:** Task-specific hints and knowledge management

**Features:**
- **Extras Registration:**
  - Add task-specific hints, guidelines, criteria
  - Type classification (pattern, definition, guideline, reference, criteria, tip)
  - Metadata tagging (category, priority, domain)
- **File Upload:**
  - Upload extras files (YAML/JSON)
  - Batch load multiple extras
- **Built-in Extras:**
  - 183+ pre-loaded clinical hints including:
    - WHO growth standards
    - ASPEN malnutrition criteria
    - Diagnostic criteria (diabetes, AKI, etc.)
    - Lab value interpretation guides
    - Medication dosing guidelines
- **Preview/Edit:**
  - View all registered extras in dataframe
  - Edit extra content and metadata
  - Remove unwanted extras
  - Search and filter extras

**Use Case:** Provide domain-specific knowledge that agents can query when needed

---

#### G. Custom Functions Tab (`ui/functions_tab.py`)
**Purpose:** Computational tools and calculator management

**Features:**
- **Function Registration:**
  - Define Python functions with parameters
  - Set parameter types and descriptions
  - Specify return value format
- **File Upload:**
  - Upload function files (YAML/JSON)
  - Batch load multiple functions
- **Built-in Functions:**
  - 40+ medical calculators including:
    - BMI, BSA, ideal body weight
    - Growth percentiles and z-scores
    - Creatinine clearance, eGFR
    - Anion gap, corrected calcium
    - Unit conversions (kgâ†”lbs, cmâ†”in, etc.)
    - Weight change percentages
    - Mean arterial pressure
    - Pack-years smoking history
- **Function Testing:**
  - Test functions with custom arguments (JSON format)
  - View function results
  - Validate execution
- **Preview/Edit:**
  - View all registered functions in dataframe
  - Edit function code and parameters
  - Remove unwanted functions
  - Export/import function definitions

**Use Case:** Provide accurate calculations that LLMs struggle with (math, dates, complex formulas)

---

#### H. Processing Tab (`ui/processing_tab.py`)
**Purpose:** Batch processing execution and monitoring

**Features:**
- **Execution Controls:**
  - Start/stop processing
  - Pause/resume support
  - Cancel processing
- **Real-time Progress:**
  - Progress bar
  - Current record indicator
  - Records/second throughput
  - Estimated time remaining
- **Live Logging:**
  - Real-time log stream
  - Error highlighting
  - Success indicators
  - Tool usage tracking
- **Error Handling:**
  - Skip on error
  - Retry failed records
  - Max retry configuration
  - Error log export
- **Batch Configuration:**
  - Batch size
  - Parallel processing
  - Checkpoint frequency
- **Results Display:**
  - Extraction success indicators:
    - âœ… SUCCESS - Extraction completed with valid JSON
    - âš ï¸ COMPLETED - Agent finished but no JSON
    - âŒ FAILED - Agent did not complete
  - Tool usage summary (RAG, Functions, Extras)
  - Export options

---

#### I. Playground Tab (`ui/playground_tab.py`)
**Purpose:** Single-text testing and debugging

**Features:**
- **Quick Testing:**
  - Paste single clinical note
  - Run extraction
  - View results immediately
- **Result Preview:**
  - Formatted JSON output
  - Extraction metadata
  - Tool calls log
- **Debugging:**
  - View LLM prompts
  - Check tool results
  - Trace execution flow
- **Iteration Testing:**
  - Test ADAPTIVE mode iterations
  - View each iteration's decisions

---

### 11. EVALUATION SYSTEM

**Directory:** `evaluation/`

**Purpose:** Measure extraction accuracy against ground truth

**Components:**

#### Metrics Calculator
**Features:**
- **Field-level Metrics:**
  - Precision, Recall, F1 score per field
  - Exact match accuracy
  - Partial match scoring
- **Aggregate Metrics:**
  - Overall F1 score
  - Average precision/recall
  - Error analysis
- **Per-record Scoring:**
  - Individual record performance
  - Error categorization

#### Evaluation Modes
```
Standard Evaluation:
â”œâ”€ Exact string matching
â”œâ”€ Numeric value comparison (with tolerance)
â”œâ”€ Boolean comparison
â””â”€ Null/missing handling

Relaxed Evaluation:
â”œâ”€ Fuzzy string matching
â”œâ”€ Case-insensitive comparison
â”œâ”€ Whitespace normalization
â””â”€ Synonym matching

Strict Evaluation:
â”œâ”€ Exact match required
â”œâ”€ Type-strict comparison
â”œâ”€ No tolerance
â””â”€ Perfect alignment required
```

**Output:**
- Detailed metrics report (CSV/JSON)
- Confusion matrices
- Error analysis by field
- Performance by execution mode

---

### 12. DATA PROCESSING UTILITIES

#### A. Growth Calculators
**Files:** `core/growth_calculators.py`, `core/cdc_growth_calculator.py`

**Purpose:** Clinical growth assessment calculations

**Features:**
- **CDC Growth Charts:**
  - Weight-for-age (0-36 months, 2-20 years)
  - Length/stature-for-age
  - BMI-for-age
  - Head circumference-for-age
  - Weight-for-stature
- **Calculations:**
  - Percentile computation (0-100)
  - Z-score calculation (-4 to +4)
  - Age-specific references
  - Sex-specific calculations
- **Data:**
  - CDC reference data included
  - LMS method (Lambda-Mu-Sigma)
  - Accurate interpolation

---

#### B. Data Processor
**File:** `core/data_processor.py`

**Purpose:** Input data parsing and validation

**Features:**
- **File Format Support:**
  - CSV (comma, tab, custom delimiters)
  - Excel (.xlsx, .xls)
  - JSON (records, objects)
- **Data Validation:**
  - Column existence checks
  - Data type validation
  - Required field verification
- **Column Mapping:**
  - Flexible column selection
  - Rename columns
  - Type conversion
- **Batch Preparation:**
  - Split into batches
  - Shuffle support
  - Sampling support

---

## ğŸ”„ HOW EVERYTHING WORKS TOGETHER

### Example End-to-End Flow: Malnutrition Extraction

```
1. USER CONFIGURATION:
   â”œâ”€ Uploads clinical_notes.csv via Data Tab
   â”œâ”€ Maps columns: text_column="clinical_note", label_column="malnutrition_label"
   â”œâ”€ Selects STRUCTURED mode via Config Tab (predictable workflow)
   â”œâ”€ Chooses GPT-4 as LLM provider
   â”œâ”€ Uploads WHO growth chart PDFs via RAG Tab
   â”œâ”€ Defines JSON schema for malnutrition fields via Prompt Tab
   â”œâ”€ Adds malnutrition-specific extras (z-score guidelines)
   â””â”€ Clicks "Start Processing" in Processing Tab

2. SYSTEM INITIALIZATION:
   â”œâ”€ App State loads all configurations
   â”œâ”€ LLM Manager establishes connection to OpenAI
   â”œâ”€ RAG Engine loads indexed growth chart documents
   â”œâ”€ Function Registry loads BMI, z-score, percentile calculators
   â”œâ”€ Extras Manager loads malnutrition interpretation hints
   â”œâ”€ Agent Factory creates ExtractionAgent (STRUCTURED mode)
   â””â”€ Data Processor loads and validates CSV

3. FOR EACH CLINICAL NOTE:

   Stage 1: TASK ANALYSIS
   â”œâ”€ Regex Preprocessor normalizes text (dates, numbers, units)
   â”œâ”€ PII Redactor removes sensitive information (if enabled)
   â”œâ”€ LLM receives:
   â”‚   â”œâ”€ Preprocessed clinical text
   â”‚   â”œâ”€ Task definition: "Extract malnutrition indicators"
   â”‚   â”œâ”€ JSON schema: {age, weight, height, bmi, z_score, percentile, ...}
   â”‚   â””â”€ Label context: "malnutrition_label=1" (OPTIONAL - only if provided)
   â”œâ”€ LLM analyzes clinical note
   â”œâ”€ LLM identifies needed information:
   â”‚   â”œâ”€ "I need to calculate BMI"
   â”‚   â”œâ”€ "I need growth percentile for age 5"
   â”‚   â”œâ”€ "I should check normal ranges from documents"
   â”‚   â””â”€ "I need z-score interpretation guidelines"
   â”œâ”€ LLM decides tool calls:
   â”‚   â”œâ”€ RAG query: "normal growth ranges for 5-year-old children"
   â”‚   â”œâ”€ Function: calculate_bmi(weight_kg=18, height_m=1.1)
   â”‚   â”œâ”€ Function: calculate_growth_percentile(weight=18, age_months=60, sex="M")
   â”‚   â””â”€ Extras: keywords=["malnutrition", "z-score", "pediatric"]
   â””â”€ Returns: Structured tool request list

   Stage 2: TOOL EXECUTION (ASYNC - All in Parallel!)
   â”œâ”€ [PARALLEL TASK 1] RAG Engine:
   â”‚   â”œâ”€ Converts query to embedding
   â”‚   â”œâ”€ Searches vector database
   â”‚   â”œâ”€ Retrieves: "For 5-year-old boys, 3rd percentile = -1.88 SD..."
   â”‚   â””â”€ Returns top-3 relevant passages (0.3s)
   â”‚
   â”œâ”€ [PARALLEL TASK 2] Function Registry - BMI:
   â”‚   â”œâ”€ Validates parameters: weight_kg=18.0, height_m=1.1
   â”‚   â”œâ”€ Executes: calculate_bmi(18.0, 1.1)
   â”‚   â”œâ”€ Returns: {"bmi": 14.88, "category": "Underweight"}
   â”‚   â””â”€ Completes in 0.1s
   â”‚
   â”œâ”€ [PARALLEL TASK 3] Function Registry - Percentile:
   â”‚   â”œâ”€ Loads CDC reference data
   â”‚   â”œâ”€ Executes: calculate_growth_percentile(18, 60, "M")
   â”‚   â”œâ”€ Returns: {"percentile": 3, "z_score": -1.88}
   â”‚   â””â”€ Completes in 0.2s
   â”‚
   â”œâ”€ [PARALLEL TASK 4] Extras Manager:
   â”‚   â”œâ”€ Matches keywords: ["malnutrition", "z-score", "pediatric"]
   â”‚   â”œâ”€ Fuzzy search across extras database
   â”‚   â”œâ”€ Returns: "CRITICAL: 3rd percentile = z-score -1.88 (moderate malnutrition)"
   â”‚   â””â”€ Completes in 0.1s
   â”‚
   â””â”€ Total Time: 0.3s (vs 0.7s if sequential) - 60% FASTER!

   Stage 3: EXTRACTION
   â”œâ”€ LLM receives comprehensive context:
   â”‚   â”œâ”€ Original clinical note (preprocessed)
   â”‚   â”œâ”€ RAG results: Growth chart references
   â”‚   â”œâ”€ Function results: BMI=14.88, percentile=3, z-score=-1.88
   â”‚   â”œâ”€ Extras: Interpretation guidelines
   â”‚   â””â”€ JSON schema: Expected output structure
   â”œâ”€ LLM extracts structured data informed by tools:
   â”‚   {
   â”‚     "patient_age_months": 60,
   â”‚     "weight_kg": 18.0,
   â”‚     "height_cm": 110.0,
   â”‚     "bmi": 14.88,
   â”‚     "growth_percentile": 3,
   â”‚     "z_score": -1.88,
   â”‚     "malnutrition_present": true,
   â”‚     "malnutrition_severity": "moderate",
   â”‚     "evidence": "Weight at 3rd percentile (z-score -1.88)"
   â”‚   }
   â”œâ”€ JSON Parser extracts and validates JSON
   â”œâ”€ Schema validation ensures all required fields present
   â””â”€ Validated JSON ready for Stage 4

   Stage 4: RAG REFINEMENT (Optional)
   â”œâ”€ Since RAG was used, perform refinement
   â”œâ”€ LLM validates extraction against RAG knowledge:
   â”‚   â”œâ”€ Confirms: "3rd percentile indicates moderate malnutrition"
   â”‚   â”œâ”€ Validates: z-score calculation correct
   â”‚   â””â”€ Verifies: interpretation aligns with guidelines
   â”œâ”€ Makes any necessary corrections
   â””â”€ Returns: Final validated JSON

4. OUTPUT GENERATION:
   â”œâ”€ Output Handler collects all extraction results
   â”œâ”€ Adds metadata:
   â”‚   â”œâ”€ timestamp: "2024-01-15T10:30:45Z"
   â”‚   â”œâ”€ model_used: "gpt-4"
   â”‚   â”œâ”€ execution_mode: "STRUCTURED"
   â”‚   â”œâ”€ rag_used: true
   â”‚   â”œâ”€ functions_called: ["calculate_bmi", "calculate_growth_percentile"]
   â”‚   â”œâ”€ extras_used: true
   â”‚   â””â”€ processing_time_seconds: 2.1
   â”œâ”€ Exports to CSV:
   â”‚   â”œâ”€ Flattened JSON columns
   â”‚   â”œâ”€ Metadata columns
   â”‚   â””â”€ Original text preserved
   â”œâ”€ Exports to Excel:
   â”‚   â”œâ”€ Sheet 1: Extraction results
   â”‚   â”œâ”€ Sheet 2: Metadata
   â”‚   â””â”€ Sheet 3: Processing logs
   â””â”€ User downloads structured dataset!

5. EVALUATION (Optional):
   â”œâ”€ If ground truth labels provided
   â”œâ”€ Evaluation system compares extraction vs truth
   â”œâ”€ Calculates:
   â”‚   â”œâ”€ Field-level F1 scores
   â”‚   â”œâ”€ Overall accuracy
   â”‚   â””â”€ Error analysis
   â””â”€ Generates metrics report
```

---

## ğŸŒŸ KEY ARCHITECTURAL PRINCIPLES

### 1. TRUE UNIVERSALITY
**No Task Hardcoding:**
- âœ… System has NO hardcoded clinical tasks
- âœ… Works for malnutrition, diabetes, renal function, cardiac assessment, etc.
- âœ… User defines task via prompts and JSON schema
- âœ… Adapts to labeled OR unlabeled data
- âœ… No code changes needed for new clinical domains

**Adaptability:**
- Every task can have different requirements
- Label context is COMPLETELY OPTIONAL
- Schema defines output structure dynamically
- Prompt defines extraction instructions
- Works across all medical specialties

---

### 2. INTELLIGENT ORCHESTRATION
**Autonomous Decision-Making:**
- âœ… LLM analyzes task and decides which tools to call
- âœ… No manual configuration of tool selection
- âœ… Context-aware tool usage
- âœ… Dynamic adaptation based on available tools

**Multi-Modal Knowledge Integration:**
- RAG: Retrieval from documents (semantic search)
- Functions: Computational tools (calculations)
- Extras: Domain-specific hints (guidelines)
- LLM: Reasoning and extraction (intelligence)

All four knowledge sources work together seamlessly!

---

### 3. DUAL EXECUTION MODES
**STRUCTURED Mode:**
- For predictable, systematic workflows
- 4-stage pipeline (Analysis â†’ Tools â†’ Extraction â†’ Refinement)
- Best for production environments
- Deterministic, repeatable

**ADAPTIVE Mode:**
- For evolving, complex tasks
- Continuous iteration with learning
- Dynamic tool calling across iterations
- Best for research and complex cases

**Important:** Both are EQUALLY autonomous and universal!

---

### 4. PERFORMANCE OPTIMIZATION
**ASYNC Tool Execution:**
- âœ… All Stage 2 tools run in PARALLEL
- âœ… 60-75% performance improvement
- âœ… Maintains execution order
- âœ… Handles failures gracefully

**Efficiency:**
- Lazy initialization (components created on demand)
- Vector database caching
- Persistent storage
- Batch processing support

---

### 5. EXTENSIBILITY
**Easy to Extend:**
- âœ… Add functions: Drop JSON file in `/functions/`
- âœ… Add extras: Drop JSON file in `/extras/`
- âœ… Add documents: Upload via RAG tab
- âœ… Add prompts: Create custom templates
- âœ… Add LLM providers: Implement provider interface

**No Code Changes Required:**
- Users extend via configuration and data files
- Developers extend via modular interfaces
- Plugin-style architecture

---

### 6. ROBUST ERROR HANDLING
**Multiple Fallbacks:**
- JSON parsing: 3 strategies (clean â†’ fuzzy â†’ fallback)
- LLM retries: Exponential backoff
- Tool failures: Graceful degradation
- Validation: Schema-based with informative errors

**Logging:**
- Detailed execution logs
- Error categorization
- Performance metrics
- Audit trail

---

## ğŸ’¡ WHAT CAN USERS DO WITH THIS PLATFORM?

### 1. Clinical Data Extraction
- Extract structured data from unstructured clinical notes
- Convert narrative text â†’ JSON/CSV
- Works for ANY clinical domain (universal)
- Supervised or unsupervised (labels optional)

### 2. Clinical Decision Support
- Query medical knowledge bases (RAG)
- Calculate clinical scores and metrics (Functions)
- Get guideline-based recommendations (Extras)
- Structured output for downstream applications

### 3. Research Data Collection
- Extract research variables from EHR notes
- Standardize clinical documentation
- Create datasets for clinical research
- Batch processing for large cohorts

### 4. Quality Improvement
- Monitor documentation completeness
- Track clinical metrics over time
- Identify documentation gaps
- Audit trail for compliance

### 5. Custom Clinical Applications
- Build task-specific extractors (no coding!)
- Create clinical calculators
- Develop knowledge bases for specialties
- Integrate with existing systems (API-ready)

### 6. Education and Training
- Demonstrate clinical reasoning
- Teach structured documentation
- Create annotated datasets
- Training data generation

---

## ğŸ” SECURITY AND COMPLIANCE

### PII Protection
- Built-in PII redaction (HIPAA-aware)
- Multiple redaction strategies
- Configurable entity types
- De-identification support

### Data Privacy
- Local processing (no data sent except to chosen LLM)
- User controls LLM provider
- Optional local models
- Audit logging

### Access Control
- API key management
- Configuration persistence
- Secure credential storage

---

## ğŸ“ˆ PERFORMANCE CHARACTERISTICS

### Throughput
- **ASYNC Tool Execution:** 60-75% faster than sequential
- **Batch Processing:** Configurable batch sizes
- **Parallel Processing:** Multiple records simultaneously (future)

### Scalability
- Vector database: Handles 100K+ document chunks
- Function registry: Unlimited functions
- Extras: Unlimited supplementary hints
- Batch size: Limited only by memory

### Latency
- Single extraction: 2-10 seconds (depends on LLM)
- RAG query: 0.1-0.5 seconds
- Function call: 0.05-0.2 seconds
- Extras query: 0.05-0.1 seconds

---

## ğŸ› ï¸ TECHNICAL STACK

### Core Dependencies
- **Python:** 3.8+
- **LLM Libraries:**
  - `openai` (OpenAI, Azure)
  - `anthropic` (Claude)
- **RAG:**
  - `faiss-cpu` (vector database - Facebook AI Similarity Search)
  - `sentence-transformers` (embeddings)
  - `torch` (transformer models)
- **NLP:**
  - `spacy` (NER for PII)
  - `transformers` (embeddings)
- **UI:**
  - `gradio` (web interface)
- **Data:**
  - `pandas` (data processing)
  - `openpyxl` (Excel support)
- **Utilities:**
  - `pydantic` (validation)
  - `asyncio` (async execution)

### Optional Dependencies
- **Local LLMs:**
  - `unsloth` (4-bit quantized models - LLaMA, Mistral, etc.)
  - `unsloth_zoo` (pre-trained model zoo)
  - `xformers` (memory-efficient transformers)
- Custom embedding models
- Additional NER models

---

## ğŸ“ PROJECT STRUCTURE

```
clinannotate/
â”œâ”€â”€ core/                          # Core system components
â”‚   â”œâ”€â”€ agent_system.py           # STRUCTURED mode agent
â”‚   â”œâ”€â”€ agentic_agent.py          # ADAPTIVE mode agent
â”‚   â”œâ”€â”€ agent_factory.py          # Agent creation
â”‚   â”œâ”€â”€ llm_manager.py            # LLM provider interface
â”‚   â”œâ”€â”€ rag_engine.py             # RAG/vector search
â”‚   â”œâ”€â”€ function_registry.py      # Computational tools
â”‚   â”œâ”€â”€ extras_manager.py         # Supplementary hints
â”‚   â”œâ”€â”€ json_parser.py            # JSON extraction
â”‚   â”œâ”€â”€ regex_preprocessor.py     # Text normalization
â”‚   â”œâ”€â”€ pii_redactor.py           # PII removal
â”‚   â”œâ”€â”€ prompt_templates.py       # Prompt management
â”‚   â”œâ”€â”€ output_handler.py         # Result export
â”‚   â”œâ”€â”€ model_config.py           # LLM configuration
â”‚   â”œâ”€â”€ app_state.py              # State management
â”‚   â”œâ”€â”€ config_persistence.py     # Config save/load
â”‚   â”œâ”€â”€ data_processor.py         # Data input handling
â”‚   â”œâ”€â”€ growth_calculators.py     # Clinical calculators
â”‚   â””â”€â”€ logging_config.py         # Logging setup
â”‚
â”œâ”€â”€ ui/                            # Gradio interface
â”‚   â”œâ”€â”€ config_tab.py             # LLM configuration
â”‚   â”œâ”€â”€ data_tab.py               # Data upload/mapping
â”‚   â”œâ”€â”€ prompt_tab.py             # Prompt templates
â”‚   â”œâ”€â”€ rag_tab.py                # Document management
â”‚   â”œâ”€â”€ processing_tab.py         # Batch processing
â”‚   â””â”€â”€ playground_tab.py         # Single-text testing
â”‚
â”œâ”€â”€ functions/                     # Function definitions (JSON)
â”‚   â”œâ”€â”€ calculate_bmi.json
â”‚   â”œâ”€â”€ calculate_weight_change.json
â”‚   â”œâ”€â”€ calculate_growth_percentile.json
â”‚   â””â”€â”€ ... (user-extensible)
â”‚
â”œâ”€â”€ extras/                        # Supplementary hints (JSON)
â”‚   â”œâ”€â”€ z_score_percentile.json
â”‚   â”œâ”€â”€ malnutrition_guidelines.json
â”‚   â””â”€â”€ ... (user-extensible)
â”‚
â”œâ”€â”€ evaluation/                    # Evaluation tools
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ datasets/
â”‚
â”œâ”€â”€ cdc_data/                      # Clinical reference data
â”‚   â””â”€â”€ growth_charts/
â”‚
â”œâ”€â”€ annotate.py                    # Main application entry
â”œâ”€â”€ ARCHITECTURE.md                # This file!
â”œâ”€â”€ README.md                      # User guide
â””â”€â”€ requirements.txt               # Dependencies
```

---

## ğŸš€ FUTURE ENHANCEMENTS

### Planned Features
- Database output (SQL, MongoDB)
- Real-time API endpoints
- Parallel batch processing
- Additional LLM providers
- Custom evaluation metrics
- Multi-language support
- Advanced PII detection
- Workflow automation

---

## ğŸ“š DOCUMENTATION

- **README.md:** Quick start guide
- **ARCHITECTURE.md:** This document - complete technical architecture
- **COMPLETE_USER_GUIDE.md:** Comprehensive user documentation
- **AGENTIC_USER_GUIDE.md:** ADAPTIVE mode detailed guide
- **PIPELINE_ARCHITECTURE.md:** Pipeline flow documentation
- **examples/:** Usage examples for different tasks

---

## ğŸ¯ DESIGN PHILOSOPHY

1. **Universal First:** No task hardcoding, adapt to anything
2. **Autonomous Intelligence:** LLM orchestrates tools intelligently
3. **User Empowerment:** Users extend without coding
4. **Performance Matters:** ASYNC, caching, optimization
5. **Robust & Reliable:** Multiple fallbacks, error handling
6. **Clear & Intuitive:** STRUCTURED vs ADAPTIVE naming
7. **Production Ready:** Logging, validation, compliance
8. **Open & Extensible:** Plugin architecture

---

**Document Version:** 1.0.0
**Last Updated:** 2024-01-15
**Architecture Version:** 1.0.0
